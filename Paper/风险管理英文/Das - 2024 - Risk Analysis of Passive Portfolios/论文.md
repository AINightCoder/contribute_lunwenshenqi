# Risk Analysis of Passive Portfolios

Sourish Das Chennai Mathematical Institute

July 2024

# Abstract

In this work, we present an alternative passive investment strategy. The passive investment philosophy comes from the Efficient Market Hypothesis (EMH), and its adoption is widespread. If EMH is true, one cannot outperform market by actively managing their portfolio for a long time. Also, it requires little to no intervention. People can buy an exchange-traded fund (ETF) with a long-term perspective. As the economy grows over time, one expects the ETF to grow. For example, in India, one can invest in NETF (see,Exchange [2022]), which suppose to mimic the Nifty50 return. However, the weights of the Nifty 50 index are based on market capitalisation. These weights are not necessarily optimal for the investor. In this work, we present that volatility risk and extreme risk measures of the Nifty50 portfolio are uniformly larger than Markowitz’s optimal portfolio. However, common people can’t create an optimised portfolio. So we proposed an alternative passive investment strategy of an equal-weight portfolio. We show that if one pushes the maximum weight of the portfolio towards equal weight, the idiosyncratic risk of the portfolio would be minimal. The empirical evidence indicates that the risk profile of an equal-weight portfolio is similar to that of Markowitz’s optimal portfolio. Hence instead of buying Nifty50 ETFs, one should equally invest in the stocks of Nifty50 to achieve a uniformly better risk profile than the Nifty 50 ETF portfolio. We also present an analysis of how portfolios perform to idiosyncratic events like the Russian invasion of Ukraine. We found that the equal weight portfolio has a uniformly lower risk than the Nifty 50 portfolio before and during the Russia-Ukraine war. All codes are available on GitHub (https://github.com/sourish-cmi/quant/tree/main/Chap_ Risk_Anal_of_Passive_Portfolio).

# 1 Introduction

In recent years, we observed several undesirable events, such as the Covid19 pandemic, the Russian invasion of Ukraine, severe global supply chain system disruption, and tremendous pressure on energy prices, followed by global inflation. These macro events bring a high degree of uncertainty to the worldwide market. Nowadays, even pension fund portfolios invest in the volatile equity market. The high degree of volatility can impact the portfolio negatively. Therefore it is essential to analyse the risk exposure that a portfolio face.

Passive investment is an excellent option if the investment horizon is long and one is not an expert in quantitative finance. The widespread adoption of the passive investment philosophy comes from the Efficient Market Hypothesis (EMH). One cannot outperform the market by actively managing their portfolio for a long time if it is efficient. One can invest in exchange-traded funds (ETF) with a long-term perspective and expects the ETF to grow as the economy grows over time. In India, we can invest in NETF (see, Exchange [2022]), which suppose to mimic the Nifty50 return. However, the weights of the Nifty 50 index are not necessarily optimal for the investors, as they are based on market capitalisation.

In this work, we showed that the Indian market is not efficient. It raises the question of how it affects the risk profile of the passive investment portfolio, particularly how it reacts to idiosyncratic events like the Russian invasion of Ukraine. We study the effect of Russian invasion on the risk profile of the portfolios.

Rest of the chapter is organised as follows. In Section (2) we present basics of financial return and volatility risk. In Section (3) we present the efficient market hypothesis and empirical evidence that shows Indian market is not efficient. In Section (4) we present different aspects of portfolio risk analysis. In Section (6), we present the empirical evidence that equal weight portfolio would have uniformly better risk profile than Nifty50 ETFs. Section (7) concludes the chapter.

# 2 On Basics of Financial Return and Volatility Risk

# 2.1 Time Value of Money

If given a choice between receiving |100 today or after a year, we should choose to receive $\yen 100$ today. Because if a bank (say State Bank of India) agrees to pay us $\yen 7$ for keeping the $\yen 100$ with them, then at the end of the period, our investment will be $\yen 107$ . This |7 is the time value of $\yen 100$ that we are going to keep with the bank. Economists term this as time preference, also known as the ‘Time Value of Money.’ The mathematical operation of evaluating the ‘present value’ (PV) of an amount into the ‘future value’ (FV) is called a capitalization. For example, how much will our |100 today be worth in 10 years? The reverse operation of evaluating the present value of a future amount of money is called the discounting. For example, how much will |100 received in 10 years, be worth today?

# 2.2 Financial Return

The goal of our any investment is to grow over time. The growth depends on both change in price and a number of the assets being invested. Certainly, our would be interested in revenues that are high compared to the initial investment. Returns articulate the change in price as a fraction of the initial price.

Suppose $P _ { t }$ is the price of an asset at time $t$ . The net return over the holding period of time $t - 1$ to $t$ is

$$
R _ { t } = { \frac { P _ { t } - P _ { t - 1 } } { P _ { t - 1 } } } = { \frac { P _ { t } } { P _ { t - 1 } } } - 1 .
$$

The numerator, $P _ { t } { - } P _ { t - 1 }$ is the net profit (or net loss) during the holding period, where denominator $P _ { t - 1 }$ is the initial investment at the start of the holding period. We can see the net returns as the rate of profit (or loss) or relative revenue on initial investment. The revenue from holding an asset is

$$
P _ { t } - P _ { t - 1 } = R _ { t } \times P _ { t - 1 } ,
$$

revenue $=$ net return $\times$ initial investment.

Example 2.1. An initial investment of |1000 and a net return of $4 \%$ over one year earn revenue of |40, which means the value of the investment after one year is |1040.

The single period gross return is defined as

$$
\frac { P _ { t } } { P _ { t - 1 } } = 1 + R _ { t }
$$

Example 2.2. If $P _ { t - 1 } = \mathfrak { F } \imath O O O$ and $P _ { t } { = } \Re { 1 0 4 0 }$ then the gross return is $1 + R _ { t } = 1 . 0 4$ or $1 0 4 \%$ and the net return is $R _ { t } = 0 . 0 4$ or $4 \%$ .

The $k$ -period gross return is the product of the $k$ single period, gross returns from time $t - k$ to time $t$ :

$$
{ \begin{array} { l c l } { 1 + R _ { t } ( k ) = { \frac { P _ { t } } { P _ { t - k } } } } & { = } & { { \Big ( } { \frac { P _ { t } } { P _ { t - 1 } } } { \Big ) } \Big ( { \frac { P _ { t - 1 } } { P _ { t - 2 } } } { \Big ) } \dots \Big ( { \frac { P _ { t - k + 1 } } { P _ { t - k } } } \Big ) } \\ & { = } & { ( 1 + R _ { t } ) ( 1 + R _ { t - 1 } ) \dots ( 1 + R _ { t - k + 1 } ) . } \end{array} }
$$

Example 2.3. Suppose in three consecutive periods (from $t$ to $t + 3 )$ the values of an asset are $P _ { t } { = } \mp 1 O O O$ , $P _ { t + 1 } = \Re { 1 0 4 0 }$ , $P _ { t + 2 } { = } \Re { 1 0 3 5 }$ , and $P _ { t + 3 } { = } \Re { 1 0 5 0 }$ .

$$
{ \begin{array} { r l } & { 1 + R _ { t + 3 } ( 1 ) = { \frac { P _ { t + 3 } } { P _ { t + 2 } } } = { \frac { 1 0 5 0 } { 1 0 3 5 } } = 1 . 0 1 4 } \\ & { 1 + R _ { t + 3 } ( 2 ) = { \frac { P _ { t + 3 } } { P _ { t + 1 } } } = { \frac { 1 0 5 0 } { 1 0 4 0 } } = 1 . 0 1 } \\ & { 1 + R _ { t + 3 } ( 3 ) = { \frac { P _ { t + 3 } } { P _ { t } } } = { \frac { 1 0 5 0 } { 1 0 0 0 } } = 1 . 0 5 } \end{array} }
$$

We see returns are independent of scale. It does not depend on the unit like rupees, dollar or pounds. However, it depends on the unit of time (like the hour, day, and year).

The log returns are defined as

$$
r _ { t } = \log ( 1 + R _ { t } ) = \log ( 1 + R _ { t } ) = \log \Big ( \frac { P _ { t } } { P _ { t - 1 } } \Big ) = p _ { t } - p _ { t - 1 } ,
$$

where $p _ { t } = \log ( P _ { t } )$ is also known as log-price or log-value of the asset. The log-returns are also known as continuously compounded returns. One advantage of the log returns is that a $k$ period log return is sum of single period log returns. That is

$$
\begin{array} { l c l } { r _ { t } ( k ) } & { = } & { \log \{ 1 + R _ { t } ( k ) \} } \\ & { = } & { \log \{ ( 1 + R _ { t } ) ( 1 + R _ { t - 1 } ) . . . ( 1 + R _ { t - k + 1 } ) \} } \\ & { = } & { \log ( 1 + R _ { t } ) + \log ( 1 + R _ { t - 1 } ) + . . . + \log ( 1 + R _ { t - k + 1 } ) } \\ & { = } & { r _ { t } + r _ { t - 1 } + . . . + r _ { t - k + 1 } . } \end{array}
$$

We can show, if $x$ is small, then $\log ( 1 + x ) \approx x$ , it means the log returns are approximately equal to net returns. Empirical studies show that the condition $\log ( 1 + x ) \approx x$ is true when $| x | < 0 . 1$ , i.e., returns that are less than $1 0 \%$ . Compounding If we have an initial investment of $P _ { t }$ that earns annual rate $r$ , compounded $m$ times a year for $n$ years then it has a future value

$$
P _ { t + 1 } = P _ { t } \Big ( 1 + \frac { r } { m } \Big ) ^ { m \times n } .
$$

If compounding times increases, then the future value will also rise. In case of continuous compounding we have

$$
P _ { t + 1 } = P _ { t } \operatorname* { l i m } _ { m  \infty } ( 1 + { \frac { r } { m } } ) ^ { m \times n } = P _ { t } e ^ { r \times n } .
$$

If $n = 1$ then continuous compounding is $\boldsymbol { P } _ { t + 1 } = \boldsymbol { P } _ { t } e ^ { \boldsymbol { r } }$ , which we can present as

$$
r = \log \left( \frac { P _ { t + 1 } } { P _ { t } } \right)
$$

the log return. Hence that the log return of an asset is known as continuously compounded rate of return.

# 2.3 Volatility as Measure of Risk

Volatility tells us, on average, how much value of an asset can go down or go up. The standard deviation calculated from the daily log returns are known as the volatility at daily levels, i.e.,

$$
\sigma = \sqrt { \mathbb { V } a r ( r _ { t } ) } .
$$

Suppose $r _ { t } , r _ { t - 1 } , \cdot \cdot \cdot , r _ { t - k + 1 }$ are $k$ single period log-return of an asset, where

$$
\begin{array} { r c l } { { \mathbb { E } ( r _ { t - i + 1 } ) } } & { { = } } & { { \mu , \forall i = 1 , 2 , \cdots , k , } } \\ { { \mathbb { V } a r ( r _ { t - i + 1 } ) } } & { { = } } & { { \sigma ^ { 2 } , \forall i = 1 , 2 , \cdots , k , } } \\ { { \mathbb { C } o v ( r _ { t - i + 1 } ) } } & { { = } } & { { 0 , \forall i \neq j = 1 , 2 , \cdots , k . } } \end{array}
$$

That is the covariance matrix is

$$
\Sigma = \left( \begin{array} { c c c c } { { \sigma ^ { 2 } } } & { { 0 } } & { { \ldots } } & { { 0 } } \\ { { 0 } } & { { \sigma ^ { 2 } } } & { { \ldots } } & { { 0 } } \\ { { \vdots } } & { { \vdots } } & { { \ldots } } & { { \vdots } } \\ { { 0 } } & { { 0 } } & { { \ldots } } & { { \sigma ^ { 2 } } } \end{array} \right) _ { k \times k }
$$

The $k$ -period return can be presented in matrix notation as

$$
\begin{array} { l l l } { { r _ { t } ( k ) } } & { { = } } & { { r _ { t } + r _ { t - 1 } + \ldots + r _ { t - k + 1 } } } \\ { { } } & { { = } } & { { c ^ { T } { \bf r } , } } \end{array}
$$

where $c ^ { T } = ( 1 , 1 , . . . , 1 ) _ { k }$ is an unit vector of order $k$ and $\mathbf { r } = \{ r _ { t } , r _ { t - 1 } , . . . , r _ { t - k + 1 } \}$ . The mean and variance of the $k$ -period return are

$$
\begin{array} { r c l } { { \mathbb { E } ( r _ { t } ( k ) ) } } & { { = } } & { { c ^ { T } \mu = k \mu , } } \\ { { \mathbb { V } a r ( r _ { t } ( k ) ) } } & { { = } } & { { c ^ { T } \Sigma c = k \sigma ^ { 2 } } } \end{array}
$$

Therefore $k$ -period volatility is $\sqrt { k } \sigma$ .

# 3 Efficient Market Hypothesis

Economists and market analysts agree that if an arbitrage opportunity exists, everybody would like to follow that strategy which would thus disturb the equilibrium. So going forward, a blanket assumption that arbitrage opportunities do not exist is being imposed. To check out more about no-arbitrage opportunity in the market, see Shreve [2004a,b].

# 3.1 Random Walk Hypothesis

Let $\mathcal { F } _ { t } = \sigma \{ P _ { j } : 0 \leq j \leq t \}$ is a $\sigma$ -field generated by $\{ P _ { 0 } , \ldots , P _ { t } \}$ , where $P _ { t }$ is the price at time point $t$ . The condition of market equilibrium can be stated in terms of conditional expected returns on the basis of $\mathcal { F } _ { t }$ , where

$$
x _ { t + 1 } = P _ { t + 1 } - \mathbb { E } ( P _ { t + 1 } | \mathcal { F } _ { t } )
$$

is the excess of market value at time $t + 1$ . It is the difference between the observed and the expected price that was projected at time $t$ on the basis of the information set $\mathcal { F } _ { t }$ . Then

$$
\mathbb { E } ( x _ { t + 1 } | \mathcal { F } _ { t } ) = 0 ,
$$

which, by definition, says that the sequence $\{ x _ { t } \}$ is a “fair game" with respect to the information sequence $\mathcal { F } _ { t }$ . Equivalently, let

$$
z _ { t + 1 } = R _ { t + 1 } - \mathbb { E } ( R _ { t + 1 } | \mathcal { F } _ { t } ) ,
$$

then,

$$
\mathbb { E } ( z _ { t + 1 } | \mathcal { F } _ { t } ) = 0
$$

so that the sequence $\{ z _ { t } \}$ is also “fair game", with respect to information sequence $\{ \mathcal { F } _ { t } \}$ . Let

$$
\omega ( \mathcal { F } _ { t } ) = \Big [ \omega _ { 1 } ( \mathcal { F } _ { t } ) , \omega _ { 2 } ( \mathcal { F } _ { t } ) , \ldots , \omega _ { p } ( \mathcal { F } _ { t } ) \Big ] ,
$$

where $\omega _ { j } ( \mathcal { F } _ { t } )$ is the amount of funds available at time $t$ that are to be invested in the $j ^ { t h }$ security, $j = 1 , 2 \dots , p$ . The total excess market value at $t + 1$ is

$$
V _ { t + 1 } = \sum _ { j = 1 } ^ { p } \omega _ { j } ( \mathcal { F } _ { t } ) [ R _ { j , t + 1 } - \mathbb { E } ( R _ { j , t + 1 } | \mathcal { F } _ { t } ) ] ,
$$

which has value

$$
\mathbb { E } ( V _ { t + 1 } | \mathcal { F } _ { t } ) = 0 ,
$$

so that the sequence $\left\{ { z } _ { t } \right\}$ is also “fair game", with respect to information sequence $\{ \mathcal { F } _ { t } \}$ . Note one thing here,

$$
\mathbb E ( R _ { t + 1 } | \mathcal F _ { t } ) = \mathbb E \Bigg ( \frac { P _ { t + 1 } - P _ { t } } { P _ { t } } \Bigg | \mathcal F _ { t } \Bigg ) ,
$$

can be expressed as

$$
\mathbb E ( P _ { t + 1 } | \mathcal F _ { t } ) = [ 1 + \mathbb E ( R _ { t + 1 } | \mathcal F _ { t } ) ] P _ { t } .
$$

If we assume in (3.1), that for all $t$ and $\mathcal { F } _ { t }$ ,

$$
\begin{array} { r } { \mathbb { E } ( P _ { t + 1 } | \mathcal { F } _ { t } ) \geq P _ { t } , \mathrm { ~ o r ~ e q u i v a l e n t l y ~ } \mathbb { E } ( R _ { t + 1 } | \mathcal { F } _ { t } ) \geq 0 , } \end{array}
$$

that means the price sequence $\{ P _ { t } \}$ for security follows a submartingale with respect to the information sequence $\mathcal { F } _ { t }$ . If (3.2) holds an equality (that is expected return and price changes are zero), then price sequence follows a martingale.

In the efficient market model, the statement that current price of a security “fully reflects" available data is assumed to imply that successive price changes are independent. Also, it is usually considered continuous changes or returns are independent and identically distributed. Together the two hypothesis constitute the random walk model.

We can make a common working assumption as the returns are mutually independent and identically distributed (i.i.d) random variables with mean $\mu$ and variance $\sigma ^ { 2 }$ . We see, for the log return,

$$
1 + R _ { t } = \exp ( r _ { t } ) \geq 0 ,
$$

which implies $R _ { t } \geq - 1$ . This satisfies the condition of limited liability, i.e., possible maximum loss is the total investment. In addition,

$$
\begin{array} { r c l } { 1 + R _ { t } ( k ) } & { = } & { ( 1 + R _ { t } ) ( 1 + R _ { t - 1 } ) \dots ( 1 + R _ { t - k + 1 } ) , } \\ & { = } & { \exp ( r _ { t } ) \exp ( r _ { t - 1 } ) \dots \exp ( r _ { t - k + 1 } ) , } \\ & { = } & { \exp ( r _ { t } + r _ { t - 1 } + \dots + r _ { t - k + 1 } ) . } \end{array}
$$

So to sum of $k$ period log-returns yield $k$ -period gross return. Now note that

$$
\frac { P _ { t } } { P _ { t - k } } = 1 + R _ { t } ( k ) = \exp ( r _ { t } + r _ { t - 1 } + . . . + r _ { t - k + 1 } ) .
$$

can be expressed as for $k = t$ ,

$$
P _ { t } = P _ { 0 } \exp ( r _ { t } + . . . + r _ { 1 } ) .
$$

Suppose $r _ { 1 } , r _ { 2 } , . . . r _ { t }$ be i.i.d with mean $\mu$ and standard deviation $\sigma$ . Let $P _ { 0 }$ be an arbitrary starting point and

$$
P _ { t } = P _ { 0 } + r _ { 1 } + r _ { 2 } + . . . + r _ { t } , t \geq 1 .
$$

The process $P _ { 0 } , P _ { 1 } , P _ { 2 } , \ldots$ is random walk and $r _ { 1 } , r _ { 2 } , \ldots$ are corresponding steps of that random walk. The conditional expectation and variance of $P _ { t }$ given $P _ { 0 }$ is $\mathbb { E } ( P _ { t } | P _ { 0 } ) = P _ { 0 } + \mu t$ and $\mathbb { V } a r ( P _ { t } | P _ { 0 } ) =$ $\sigma ^ { 2 } t$ . The parameter $\mu$ is the drift and set an overall trend of the random walk. The parameter $\sigma$ is the volatility and controls how much it fluctuates around $P _ { 0 } + \mu t$ . Since the standard deviation of $P _ { t }$ given $P _ { 0 }$ is $\sigma { \sqrt { t } }$ , as $t$ increases the range of variability in the process increases. This means at the $t = 0$ we know very little about where the random walk will be in the remote future compared to its current spot value.

Therefore, if the log returns are assumed to be i.i.d. random variables, then the price of the stock or market index, denoted by the process $P = \{ P _ { t } : t \geq 0 \}$ , is the exponential of random walk or also known as the geometric random walk.

# 3.2 Test for Random Walk Hypothesis

If the price of a stock follows the geometric random walk, then we can write the log-return as

$$
p _ { t } = p _ { t - 1 } + r _ { t } ,
$$

where $p _ { t } = \log ( P _ { t } )$ and $r _ { t }$ follows the same distribution with drift parameter $\mu$ and volatility parameter $\sigma ^ { 2 }$ . The random walk is said to have unit root. To understand what this means, we should consider the AR(1) model (i.e., Auto-Regressive model with lag 1),

$$
p _ { t } = \phi p _ { t - 1 } + r _ { t }
$$

where $\phi = 1$ . The generic AR(1) model can be presented as

$$
\begin{array} { l l l } { { p _ { t } } } & { { = } } & { { \displaystyle \phi p _ { t - 1 } + r _ { t } } } \\ { { } } & { { = } } & { { \displaystyle \phi ( \phi p _ { t - 2 } + r _ { t - 1 } ) + r _ { t } } } \\ { { } } & { { = } } & { { \displaystyle \phi ^ { 2 } p _ { t - 2 } + \phi r _ { t - 1 } + r _ { t } } } \\ { { } } & { { \vdots } } & { { } } \\ { { } } & { { = } } & { { \displaystyle \phi ^ { k } p _ { t - k } + \phi ^ { k - 1 } r _ { t - ( k - 1 ) } + . . . + \phi r _ { t - 1 } + r _ { t } } } \\ { { } } & { { = } } & { { \displaystyle \phi ^ { k } p _ { t - k } + \sum _ { i = 0 } ^ { k - 1 } \phi ^ { i - 1 } r _ { t - ( i - 1 ) } . } } \end{array}
$$

• If $\phi = 1$ then the process is non-stationary. Because $\scriptstyle \sum _ { i = 0 } ^ { k - 1 } \phi ^ { i - 1 } r _ { t - ( i - 1 ) }$ accumulates the information over time. Hence a random walk is a non-stationary process.

• However $| \phi | < 1$ , i.e., $- 1 < \phi < 1$ implies, the process is stationary.

• If $\phi = 0$ that means the process is stationary and $p _ { t }$ and $p _ { t - 1 }$ are independent $\forall t$ .

We may ask here what a stationary process is? How does it look? How it looks different from the non-stationary process?

As the series $\{ p _ { t } : t \geq 0 \}$ is a random walk (i.e., $\phi = 1 \mathrm { \dot { \Omega } }$ ) the incremental steps (i.e., log-returns) are independent and stationary process, we can write it as

$$
r _ { t } = \phi _ { 1 } r _ { t - 1 } + \epsilon _ { t }
$$

where $\phi _ { 1 } = 0$ and $\epsilon _ { t }$ is white noise with mean $\mu$ and variance $\sigma ^ { 2 }$ .

In order to check if the price of a stock follows the geometric random walk, we have to check following three things.

1. First, we should check if $\{ p _ { t } \}$ is a non-stationary process, i.e.,

$$
p _ { t } = \phi p _ { t - 1 } + r _ { t } ;
$$

check if $\phi = 1$ or $\phi < 1$ .

2. Second, we check if the log-returns are stationary process, i.e.,

$$
r _ { t } = \phi _ { 1 } r _ { t - 1 } + \epsilon _ { t } ;
$$

check if $\phi _ { 1 } = 1$ or $\phi _ { 1 } < 1$ .

3. Second check only tells we if a log-returns are stationary, but it does not check if $\phi _ { 1 } = 0$ or not. In addition $\phi _ { 1 } = 0$ only implies pairwise independence. It does not check the mutual independence of $r _ { t }$ . So we should check if the serial correlations of $r _ { t }$ are 0 or not. That is check if $\rho _ { 1 } = \rho _ { 2 } = . . . = \rho _ { H } = 0$ , where $\rho _ { h } = c o r r ( r _ { t } , r _ { t + h } )$ is the lag $h$ auto-correlation.

# 3.3 Dickey-Fuller test for Stationarity in a Return Series

A test involving much more narrowly-specified null and alternative hypotheses was proposed by Dickey and Fuller [1979]. The test compares the null hypothesis

$$
H _ { 0 } : p _ { t } = p _ { t - 1 } + r _ { t }
$$

i.e., that the series is a random walk without drift, where $r _ { t }$ is a white noise with mean 0 and variance $\sigma ^ { 2 }$ . The alternative hypothesis is

$$
H _ { 1 } : p _ { t } = \mu + \phi p _ { t - 1 } + r _ { t }
$$

where $\mu$ and $\phi$ are constant with $| \phi | < 1$ . According to $H _ { 1 }$ , the process is stationary AR(1) with mean $\frac { \mu } { 1 - \phi }$ . We implement the Dickey Fuller test using adf.test function in tseries package.

Note that first we have to check if the log prices are a random walk. Then we have to check if the log-returns are also a random-walk or if the log-return follows the stationary distribution. In these two cases, we can use Dickey-Fuller test. Then on the third step, we check if consecutive log-returns are independent or not!

# 3.4 Ljung-Box test for independence in a Return Series

We can check the independence of log-return between the consecutive days via Ljung-Box test (see, Ljung and Box [1978]) for autocorrelation. Suppose the correlation between $r _ { t }$ and $r _ { t + h }$ is denoted as $\rho _ { h } = c o r r ( r _ { t } , r _ { t + h } )$ and known as lag $h$ auto-correlation. The null hypothesis is $\rho _ { h } = 0$ for. That is,

$$
H _ { 0 } : \rho _ { 1 } = \rho _ { 2 } = . . . = \rho _ { H } = 0 \ \forall \ t ,
$$

vs.

$H _ { 1 } : \mathrm { A t }$ least one inequality.

The test statistic for the Ljung-Box test is

$$
Q = n ( n + 2 ) \sum _ { h = 1 } ^ { H } \frac { { \hat { \rho } } _ { h } ^ { 2 } } { n - h } ,
$$

where $n$ is the sample size, $\hat { \rho } _ { h }$ is the sample autocorrelation of lag $h$ . We can show under $H _ { 0 }$ , $Q$ follows a chi-square distribution, $\chi _ { ( h ) } ^ { 2 }$ . The Ljung-Box test can be done in $\mathrm { R }$ using Box.test function available in stats package.

# 3.5 Test for Efficient Hypothesis with R

Market analysts always want to know if the market is efficient? Here we see; how we can check and test if the market is efficient. Since EMH is a “hypothesis"; therefore we can run a statistical test to check whether EMH is true! We can consider the market index as proxy or representative of the market as a whole and check if the market index is following a random walk. If it follows a random walk, then it is good enough to claim that market is efficient.

We consider the adjusted close value of Nifty50 market index of National Stock Exchange. We can download data using the R-package the quantmod. We consider the adjusted close value, and compute the log return of the Nifty. First, plot values of Nifty50 and log return over time in Figure (1).

Plot Nifty50 values and its log-return over time.

library(quantmod)   
library(tseries)   
## Download Nifty50 from Yahoo   
getSymbols("^NSEI",src $=$ "yahoo")   
Nifty50 $=$ NSEI\$NSEI.Adjusted   
plot(TCS)

### calculate and plot the log-return log_return $=$ diff(log(TCS)) $\star 1 0 0$ plot(log_return)

We presented both plot in the Figure (1).

![](images/75ff208a75be9363b43c718c6847bed85f2b886d51e8fcb12693fcbe8dc01bab.jpg)  
Figure 1: Values of Nifty 50 and log return over time. We consider the log-return as log-return $=$ $\log ( P _ { t } / P _ { t - 1 } ) \times 1 0 0$

We try to answer the following questions to check if the Indian market is efficient.

1. Are the values of Nifty50 non-stationary?

2. Are the log-returns of Nifty50 non-stationary?

3. Are the log-returns uncorrelated?

4. Do the log-returns follow Gaussian distribution?

# Step 1: Check if values of Nifty 50 is non-stationary

## Augmented Dickey-Fuller (adf) test for unit-root   
$>$ library(tseries)   
$>$ adf.test(na.omit(Nifty50))   
data: na.omit(Nifty50)   
Dickey-Fuller $= - 2 . 1 4 6 3$ , Lag order $\qquad = \quad \mathtt { 1 5 }$ , p-value $=$ 0.5164   
alternative hypothesis: stationary

Inference: Fail to reject null hypothesis. That is Nifty 50 values are non-stationary.

Step 2: Check if log-returns are non-stationary with Dickey-Fuller test $>$ adf.test(na.omit(log_return))

data: na.omit(log_return)   
Dickey-Fuller $= \ - 1 4 . 3 2 7$ , Lag order $\qquad = \quad \mathtt { 1 5 }$ , p-value = 0.01   
alternative hypothesis: stationary

Inference: We reject the null hypothesis. That is log-returns of Nifty 50 are stationary.

Step 3: Check if the log-returns are uncorrelated with Ljung-Box test.

$>$ Box.test(log_return, $\mathtt { l a g = 1 0 }$ ,type $=$ "Ljung-Box") data: log_return X-squared $= 3 7 . 2 3 4$ , $\mathsf { d } \pounds \ = \ 1 0$ , p-value = 5.155e-05

Inference: We reject null hypothesis as $\mathrm { p }$ -value is significantly small. That is log-returns of Nifty 50 are correlated.

Step 4: Check if the log-returns are Normal with Shapiro-Wilk test for normality, (see Shapiro and WILK [1965]).

## Shapiro-Wilk test for normality   
## Null Hypothesis: log-return follows Normal distribution   
## Alternative Hypothesis : log-return does not   
## follow a normal distribution   
$>$ shapiro.test(as.vector(log_return))   
data: as.vector(log_return)   
W = 0.89425, p-value $<$ 2.2e-16

Inference: We reject null hypothesis as p-value is significantly small. That is log-return of Nifty 50 does not follow Gaussian distribution.

We draw histogram and qqplot of log-return and presented in Figure (2).

## Draw histogram of log-return of the FTSE   
hist(log_return,main ${ \bf \Phi } . = { \bf \Phi }$ "",col $=$ "blue",nclass $=$ 20,probability $=$ TRUE)   
qqnorm(log_return,xlim $= _ { \mathsf { C } }$ (-4,4) ,ylim $_ { 1 } = _ { \mathrm { { C } } }$ (-4,4) , $\mathsf { C e x } { = } 0 \ldots 3$ )   
abline( $\mathtt { a } = 0$ , $\mathrm { b } { = } 1$ ,col $=$ "blue")   
grid(col $=$ "red")

In conclusion, the Indian stock market is not efficient, as the market returns are correlated and do not follow the Gaussian distribution. However, the log returns are stationary.

# 3.6 Capital Asset Pricing Model

As an investor, we would like to assess if the price of a stock is less than its expected level. If we see the stock is already overpriced, then the chance that it will appreciate further will be less, and we would like to sell the stock. Other investors would also like to sell the stock as we have the same information. The stock will fall back to its expected level. On the contrary, if the stock is underpriced, many investors would like to buy it with the assumption that the price will rise to its expected level. In finance, this is known as ‘Asset Pricing,’ and corresponding mathematical models are known as ‘Capital Asset Pricing Models’ (CAPM). It explains the expected risk and returns for a single asset or portfolio. We can estimate the risk premium of assets using the CAPM.

![](images/05ce9360569c59ec26d6466ba9c2378f8ad2574f0004508e42888f09bef34764.jpg)  
Figure 2: (a) Histogram of log-return of Nifty50, and (b) The qq-norm plot of log-return indicates that the log-return of Nifty 50 certainly does not follow the Gaussian distribution.

Suppose $r p _ { t j } = ( r _ { t j } - r _ { f } )$ is excess return over the $r _ { f }$ (i.e., risk free rate) for the $j ^ { t h }$ stocks on the $t ^ { t h }$ day. The $r p _ { j }$ is also known as risk premium of the ${ \bf { \bar { \mathbf { \rho } } } } _ { j } t h $ stock. We consider the model as follows:

$$
\begin{array} { r } { \pmb { r } = \pmb { X } \beta + \pmb { \varepsilon } , } \end{array}
$$

where $\pmb { r } = ( ( r p _ { t j } ) ) _ { n \times P }$ is the matrix of risk-premium for $P$ many assets that are available in the market over $n$ days; $X \beta$ is the systematic return due to market index, where

$$
\boldsymbol { X } = ( ( 1 , r _ { m } ) ) _ { n \times 2 } ,
$$

is the design matrix with the first column being the unit vector or the place holder for intercept and the second column being the risk-premium for the market index over the risk free rate $r _ { f }$ ;

$$
\beta = \left( { \begin{array} { c c c c } { \alpha _ { 1 } } & { \alpha _ { 2 } } & { \cdot \cdot \cdot } & { \alpha _ { P } } \\ { \beta _ { 1 } } & { \beta _ { 2 } } & { \cdot \cdot \cdot } & { \beta _ { P } } \end{array} } \right) _ { 2 \times P } .
$$

If the market is efficient then according to (see, Sharpe [1964],Black [1972]), then $\alpha _ { i } = 0 \forall i =$ $1 , 2 , \cdots , P$ and $\beta _ { i }$ is the measure of systematic risk due to market movement; $\ v { \varepsilon } = ( ( \varepsilon _ { t j } ) ) _ { n \times P }$ is the idiosyncratic return of the asset. In general one can consider a $k$ -factor model with $\boldsymbol { X }$ being $n \times ( k + 1 )$ dimensional, where the first column of $\boldsymbol { X }$ is constant and other columns are all suitable factors. The coefficients $\beta$ is $( k + 1 ) \times P$ dimensional and we shall denote it as

$$
\beta = ( \theta _ { 1 } , \cdot \cdot \cdot , \theta _ { P } ) , \quad \mathrm { w h e r e } \quad \theta _ { i } = ( \alpha _ { i } , \beta _ { i } , b _ { i } ^ { ( 1 ) } , \cdot \cdot \cdot , b _ { i } ^ { ( k - 1 ) } ) , i = 1 , 2 , \cdot \cdot \cdot , P .
$$

Remark 3.1. Equation (3.3), for the $k$ factor model, if $a$ portfolio is constructed based on $\tilde { P }$ many assets all of which have $\alpha = 0$ , $\beta = 1$ and $b ^ { ( j ) } = 0 \forall j \in \{ 1 , 2 , \cdot \cdot \cdot , k - 1 \}$ , then the portfolio return will mimic the market return.

The covariance of $\mathbf { \nabla } _ { \mathbf { r } _ { t } }$ is $\pmb { \Sigma }$ which can be decomposed into

$$
\begin{array} { r } { \pmb { \Sigma } = \beta ^ { T } \pmb { \Sigma } _ { X } \beta + \pmb { \Sigma } \pmb { \varepsilon } , } \end{array}
$$

where $\Sigma _ { \pmb { \varepsilon } } = d i a g ( \sigma _ { 1 } ^ { 2 } , \sigma _ { 2 } ^ { 2 } , \cdots , \sigma _ { P } ^ { 2 } )$ and $\Sigma _ { X }$ is the covariance matrix of $\boldsymbol { X }$ . In the next section we develop portfolio optimisation using the CAPM strategy.

# 4 Portfolio Risk Analysis

# 4.1 Portfolio Selection by Minimising Idiosyncratic Risk

Let us consider a portfolio $\omega = \{ \omega _ { 1 } , \omega _ { 2 } , \cdot \cdot \cdot , \omega _ { P } \}$ where $\omega _ { i } \geq 0$ $, i = 1 , \cdots , P , \sum _ { i = 1 } ^ { P } \omega _ { i } = 1$ . Markowitz’s portfolio optimization (see, Markowitz [1952]) can be expressed as the following quadratic programming problem:

$$
\operatorname* { m i n } _ { \boldsymbol { \omega } } ~ \boldsymbol { \omega } ^ { T } \boldsymbol { \Sigma } \boldsymbol { \omega } , ~ \mathrm { s u b j e c t } ~ \mathrm { t o } ~ \boldsymbol { \omega } ^ { T } \mathbf { 1 } _ { P } = 1 ~ \mathrm { a n d } ~ \boldsymbol { \omega } ^ { T } \boldsymbol { \mu } = \boldsymbol { \mu } _ { k } .
$$

Here , $\mathbf { 1 } _ { P }$ is a $P$ -dimensional vector with one in every entry and $\mu _ { k }$ is the desired level of return. The portfolio covariance can be decomposed into two parts as,

$$
\begin{array} { r c l } { \omega ^ { T } \Sigma \omega } & { = } & { \omega ^ { T } [ \beta ^ { T } \Sigma _ { X } \beta + \Sigma _ { \mathcal { E } } ] \omega } \\ & { = } & { \omega ^ { T } \beta ^ { T } \Sigma _ { X } \beta \omega + \omega ^ { T } \Sigma _ { \mathcal { E } } \omega , } \end{array}
$$

where first part explains the portfolio volatility due to market volatility and the second part explains portfolio volatility due to idiosyncratic behaviour of the stock. We assume $\sigma _ { i } ^ { 2 }$ ’s are bounded $\forall i$ .

Result 4.1. Under the CAPM model (3.3), covariance matrix (3.4) and assumption (4.1), if $P \longrightarrow \infty$ , and $M _ { \omega : P } = \operatorname* { m a x } \{ \omega \} \longrightarrow 0$ and $\sigma _ { \mathrm { m a x } } ^ { 2 } = \operatorname* { m a x } \{ \Sigma \varepsilon \} < \infty$ , then

$$
\operatorname* { l i m } _ { P \longrightarrow 0 } \omega ^ { T } \Sigma \epsilon \omega = 0
$$

Proof. We consider,

$$
\begin{array} { r c l } { \displaystyle \omega ^ { T } \Sigma _ { \epsilon } \omega } & { = } & { \displaystyle \sum _ { i = 1 } ^ { P } \omega _ { i } ^ { 2 } \sigma _ { i } ^ { 2 } } \\ & { \le } & { \displaystyle \sigma _ { \operatorname* { m a x } } ^ { 2 } \sum _ { i = 1 } ^ { P } \omega _ { i } ^ { 2 } , \sigma _ { \operatorname* { m a x } } ^ { 2 } = \operatorname* { m a x } \{ \Sigma _ { \epsilon } \} , } \\ & { \le } & { \displaystyle \sigma _ { \operatorname* { m a x } } ^ { 2 } M _ { \omega : P } \sum _ { i = 1 } ^ { P } \omega _ { i } , M _ { \omega : P } = \operatorname* { m a x } \{ \omega \} , } \\ & { = } & { \displaystyle \sigma _ { \operatorname* { m a x } } ^ { 2 } M _ { \omega : P } . } \end{array}
$$

Clearly, if $P \longrightarrow \infty$ and ${ \cal M } _ { \omega : { \cal P } } \longrightarrow 0 \implies \omega ^ { T } \Sigma _ { \epsilon } \omega \longrightarrow 0$

Remark 4.1. The Result (4.1) is based on real analysis and not probabilitic result. A detailed discussion about the Result (4.1) can be found in Das and Sen [2020].

Result 4.2. If $P$ is fixed, then $\begin{array} { r } { M _ { \omega : P } \ge \frac { 1 } { P } } \end{array}$ . If we want to minimise $M _ { \omega : P }$ , then the we must have equal weights portfolio, i.e., $\begin{array} { r } { M _ { \omega : P } = \frac { 1 } { P } } \end{array}$ . So

$$
i f M _ { \omega : P } = \frac { 1 } { P } , \ t h e n \omega ^ { T } \Sigma _ { \epsilon } \omega
$$

is minimum.

Remark 4.2. The idiosyncratic risk of the portfolio will be washed out, as the portfolio’s size increases and the portfolio’s maximum weight is bounded. The portfolio’s performance will be a function of the systematic risk explained by the market indices.

Remark 4.3. The above result mathematically demonstrates the proverb, "one should not put all eggs in one basket."

Remark 4.4. Thus we can select $\tilde { P } ( \ll P )$ many assets for the portfolio (out of $P$ many assets available in the market) such that the idiosyncratic risk becomes negligible, i.e., $\forall \delta > 0 , \bar { \exists } \tilde { P } _ { \delta }$ such that for $\tilde { P } > \tilde { P } _ { \delta }$ ,

$$
\omega _ { \tilde { P } } ^ { T } \Sigma _ { \tilde { P } } \omega _ { \tilde { P } } < \delta ;
$$

and portfolio return is mostly explained by $\theta$ only. Note that here $\tilde { P }$ is the effective size of the portfolio.

Remark 4.5. Oracle Set: Suppose the market is not efficient and there are $q$ many assets whose $\alpha > 0$ , where $q < { \tilde { P } } \ll P .$ . Let us call this set as $A _ { q }$ . We can construct $a$ portfolio with $\tilde { P }$ many assets, such that

$$
\begin{array} { r } { \mathbb { P } ( A _ { q } \subset B _ { \tilde { P } } ) \geq 1 - \eta , } \end{array}
$$

where $B _ { \tilde { P } }$ is the set of assets in the portfolio, $0 \leq \eta \leq 1$ and $\omega _ { \tilde { P } } ^ { T } \pmb { \Sigma } _ { \tilde { P } } \omega _ { \tilde { P } } < \delta$

Example 4.1. Suppose the market consists of $P = 2 0 0 0$ stocks and $a$ portfolio manager wants to build the portfolio with $\tilde { P } = 1 0 0$ stocks. If $q = 5$ many stocks are available with $\alpha _ { j } > 0 , \ j = 1 , 2 , 3 , 4 , 5$ , then the portfolio manager would like to build a portfolio, such that $A _ { q = 5 }$ is the subset of manager’s selected portfolio $B _ { \tilde { P } = 1 0 0 }$ . In other words, the manager wants to build her portfolio in such $a$ way that she does not want to miss out the set of five under-valued stocks $A _ { q = 5 }$ . That is, she wants to employ a statistical methodology, where $\mathbb { P } ( A _ { q = 5 } \subset B _ { \tilde { P } = 1 0 0 } )$ would be very high. Note that if the market is efficient, then $A _ { q }$ will be a null set.

The problem reduces to identifying the oracle set $A _ { q }$ . Essentially, it is a multiple testing problem, where we select those stocks in the portfolio $B _ { \tilde { P } }$ for which we reject the following null hypothesis:

$$
H _ { 0 i } : \theta _ { i } = \theta _ { 0 } \quad v s . \quad H _ { 1 i } : \theta _ { i } \neq \theta _ { 0 } , \quad i = 1 , 2 , \cdot \cdot \cdot , P .
$$

where $\theta _ { 0 } = ( 0 , 1 , 0 , \cdots , 0 )$ . Das and Sen [2020] presented optimal test such that Equation (4.2) is satisfied.

# 4.2 Regularising Portfolio Risk Analysis

It is important to estimate the volatility and determine the primary sources of volatility. Often the number of assets of well-diversified mutual funds or pension funds is more than thousands. However, portfolio managers are concerned about the stationarity in long time-series data. They are interested only in recent volatility, which considers daily returns of a month and sometimes even less. As the number of assets $( P ) \mathrm { i n }$ a portfolio is greater than the number of days of return $( n )$ , the rank of the covariance matrix is less than complete; such cases yield non-unique solutions. Generally, it is known as the “ill-posed" problem.

So for large portfolio, as $P \longrightarrow \infty$ and $n \longrightarrow \infty$ , however $P / n = \lambda$ , where $0 < \lambda < 1$ is a constant; the sample covariance matrix

$$
\mathbf { S } = { \frac { 1 } { n - 1 } } \sum _ { i = 1 } ^ { n } ( \pmb { r } _ { i } - { \bar { r } } ) ( \pmb { r } _ { i } - { \bar { r } } ) ^ { T } ,
$$

where $\boldsymbol { r } _ { i } = ( r _ { i 1 } , \cdot \cdot \cdot , r _ { i P } ) ^ { T }$ , $\bar { \boldsymbol { r } } = ( \bar { r } _ { 1 } , \cdots , \bar { r } _ { P } ) ^ { T }$ , and $\textstyle { \bar { r } } _ { j } = { \frac { 1 } { n } } \sum _ { i = 1 } ^ { n } r _ { i j }$ , $j = 1 , 2 , \cdots , P$ . Note that $r a n k ( \mathbf { S } ) =$ ${ \mathrm { m i n } } ( n , p ) = n < p$ , where S is a $p \times p$ matrix. In such cases, there are two problems. First, as the sample covariance matrix $\mathbf { S }$ is less than full rank, the sampling distribution of S is degenerate. Hence no valid statistical inference can be implemented with such S. Second, as we try to implement Markowitz’s portfolio optimization, as described in Equation (4.1), the portfolio covariance matrix $\pmb { \Sigma }$ is unknown. However, we cannot estimate it with S, as S is not a full rank. The Markowitz optimization (4.1) would not have any unique solution, if we use S. To address this issue one can use the shrinkage estimator for covariance (see, Ledoit and Wolf [2003]) for portfolio optimization. However, the sample distribution of these estimators are not completely understood. Hence one cannot run the statistical inference using these shrinkage estimators. Das et al. [2017] and Das and Dey [2010] presented the Bayesian inference for covariance matrix, particularly when S is less than full rank matrix. Das et al. [2017] considered that inverse Wishart prior for $\pmb { \Sigma }$ , while S follow Wishart distribution, i.e.,

$$
\begin{array} { r l r } { \Sigma } & { \sim } & { { \mathcal W } ^ { - 1 } ( n _ { 0 } , { \bar { \Psi } } ) , } \\ { { \bf S } } & { \sim } & { { \mathcal W } ( n - 1 , { \bf \Sigma } ) , } \end{array}
$$

where $\Psi$ is a postive definite matrix and the posterior distribution of $\pmb { \Sigma }$ is

$$
\Sigma | \mathbf { S } \sim \mathcal { W } ^ { - 1 } ( n _ { 0 } + n - 1 , \Psi + \mathbf { S } ) .
$$

If $n < P$ , then we choose the prior degrees of freedom as

$$
\begin{array} { r } { n _ { 0 } = ( P - n ) + c , } \end{array}
$$

where $c > 0$ . This ensures posterior distribution to be proper. The posterior mode of $\pmb { \Sigma }$ is

$$
M ( \pmb { \Sigma } | \mathbf { S } ) = q \frac { \pmb { \Psi } } { n _ { 0 } + P + 1 } + ( 1 - q ) \frac { \pmb { \mathrm { \bf ~ S } } } { n - 1 } ,
$$

where $\begin{array} { r } { q \ = \ \frac { n _ { 0 } + p + 1 } { n _ { 0 } + n + p } } \end{array}$ . The posterior mode of $\pmb { \Sigma }$ is a shrinkage estimator, a weighted average of prior distribution’s mode and sample covariance estimator. The advantage of Das et al. [2017] is that full posterior distribution of $\pmb { \Sigma }$ is known. Hence we can run full Bayesian inference on $\pmb { \Sigma }$ . In addition, since posterior mode of $\pmb { \Sigma }$ is positive definite and full rank, we can run portfolio optimization and further portfolio risk analysis.

# 4.3 Bayesian Inference with Regularised Covariance

The portfolio return over the period is

$$
\begin{array} { r } { r _ { p } = r \omega , } \end{array}
$$

where $r _ { p } = \{ r _ { p 1 } , \cdot \cdot \cdot , r _ { p n } \} ^ { T }$ , $\pmb { r } = ( ( r p _ { t j } ) ) _ { n \times P }$ is the risk-premium matrix, and ${ \boldsymbol \omega } = \{ \omega _ { 1 } , \cdot \cdot \cdot , \omega _ { P } \} ^ { T }$ , such that $\omega _ { j }$ is the $j ^ { t h }$ asset’s weight. The portfolio volatility is defined as

$$
\sigma _ { P } = \sqrt { \omega ^ { T } \Sigma \omega } .
$$

The portfolio weights $\omega$ , and the covariance structure of the portfolio plays a crucial role as regulators of the portfolio’s total volatility $\sigma _ { P }$ . However, it is also essential to quantify how sensitive the portfolio volatility is concerning a slight change in $\omega$ . We can achieve it by differentiating the volatility with respect to weight, and it is known as the ’Marginal Contribution to Total Risk’ (MCTR), see Menchero and Davis [2011] and Baigent [2014].

Definition 4.1. The ‘Marginal Contribution to Total Risk’ (MCTR) is defined as

$$
\frac { \partial ( \sigma _ { P } ) } { \partial \omega } = \frac { 1 } { \sigma _ { P } } . \Sigma . \omega = \varrho ,
$$

where $\pmb { \varrho } = \{ \varrho _ { 1 } , \cdot \cdot \cdot , \varrho _ { P } \}$ , . The MCTR for asset $i$ is

$$
\varrho _ { i } = \frac { 1 } { \sigma _ { P } } \sum _ { j = 1 } ^ { P } \sigma _ { i j } \omega _ { j } .
$$

Definition 4.2. The ‘Conditional Contribution to Total Risk’ (CCTR) is the amount that an asset that contributes to the total portfolio volatility. In other words, if $\mathsf { f } \zeta _ { j } = \omega _ { j } \varrho _ { j }$ is the CCTR of the asset $j$ then

$$
\sigma _ { P } = \sum _ { j = 1 } ^ { P } \zeta _ { j } = \sum _ { j = 1 } ^ { P } \omega _ { j } \varrho _ { j } .
$$

Therefore the total volatility is weighted average of the MCTR.

Remark 4.6. A regularized estimate of $\pmb { \Sigma }$ is required to estimate the MCTR and CCTR, while the weights are fixed. However, for all practical purposes we are interested in estimation of $\mathbb { P } ( \zeta < 0 )$ or $\mathbb { P } ( \varrho < 0 )$ . Because negative CCTR or MCTR of an asset means that asset actually reduces the volatility.

In Section (4.2), we presented that the posterior distribution of $\pmb { \Sigma }$ follows $\mathcal { W } ^ { - 1 } ( n _ { 0 } + n - 1 , \Psi + \mathbf { S } )$ To estimate the contribution to risk, we present the following Monte Carlo algorithm.

# Algorithm 1: Bayesian Monte Carlo Algorithm to Estimate Contribution to Risk

Remark 4.7. In the Algorithm (1), each iterations are independent. Hence parallel implementation of the algorithm is very simple. In fact we can consider the algorithm to be an ‘embarrassingly parallel’, see Matloff [2011].

Remark 4.8. Implementing the algorithm in parallel might not be required if $P$ is small. However, as $P$ is large, generating $\pmb { \Sigma } ^ { ( i ) }$ will be slow for a large portfolio. Thus, consequent computation in all the other steps will also be slow. In such cases, parallelization of the algorithm improves the time performance of the algorithm.

The MC estimate of CCTR and other risk metrics can be estimated, like

$$
\mathbb { P } ( \zeta _ { j } > 0 ) = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \mathbb { I } ( \zeta _ { j } ^ { ( i ) } > 0 ) ,
$$

once the MC samples are generated.

# 4.4 Analysing the Extreme Risk of Portfolio

The volatility risk is a measure of average risk of the portfolio. However, it does not say anything about the risk of large losses, or extreme risk. Value at Risk (VaR) of a portfolio is a measure of extreme risk. It states that the portfolio will lose more than a large amount is the $\alpha$ quantile of the portfolio return, i.e.,

$$
\begin{array} { r } { V a R _ { \alpha } ( r _ { P } ) = - \operatorname* { i n f } \{ v : \mathbb { P } ( r _ { p } < v ) \leq \alpha \} = F _ { r _ { P } } ^ { - 1 } ( \alpha ) , } \end{array}
$$

where $v$ is the loss of the portfolio, and $\alpha \in ( 0 , 1 )$ is the confidence level.

Example 4.2. If a portfolio of stocks has a one-day $1 \%$ VaR of $\yen 1$ crore, there is $1 \%$ probability that the portfolio will decline in value by more than $\yen 1$ crore over the next day.

Remark 4.9. The VaR provides a measure of how much extreme financial risk we are exposed to. It provides a structured methodology for critically thinking about risk, and consolidating risk across an organization. VaR can be applied to individual stocks, portfolios of stocks, gold, and other commodity, etc.

Remark 4.10. One has to have a distributional assumption about $F$ . Instead of assuming a particular parametric distribution, we considered empirical distribution.

The VaR is a frequency measure. It does not measure the expectation of the amount lost. The Expected Shortfall (ES), aka., the Conditional VaR or CVaR, is the measure of risk of expected amount of large loss, i.e.,

$$
E S _ { \alpha } = \mathbb { E } \big ( r _ { p } | r _ { p } < - V a R _ { \alpha } ( r _ { p } ) \big ) = \int _ { - \infty } ^ { - V a R _ { \alpha } } r _ { p } d F ( r _ { p } ) .
$$

Remark 4.11. A coherent risk measure satisfies four properties, i.e., (i) monotonicity, (ii) subadditivity, (iii) homogeneity, and (iv) translational invariance. The VaR is not a coherent risk measure. However, ES is $a$ coherent risk measure. It makes ES a more desirable risk measure than VaR.

# 5 Nonparametric Bootstrap Methods in Risk Analysis

Nonparametric Bootstrap statistics is an algorithmic approach that typically employs a simple random sample with replacement (SRSWR) scheme. It belongs to the broader category of resampling strategies. Bootstrap was introduced by . Despite its apparent simplicity, the concept revolutionised statistics by replacing analytical derivations with brute computational power. Moreover, in cases where parametric assumptions are known to be incorrect, Nonparametric Bootstrap provides an appropriate solution. For example, in the Capital Asset Pricing Model (CAPM), the underlying distribution is often assumed to be Gaussian, which is not accurate, as evidenced by Figure (2).

The nonparametric bootstrap method involves resampling returns from a given set of asset returns. Suppose $\pmb { r } = \{ r _ { 1 } , r _ { 2 } , \cdots , r _ { n } \}$ represents the log-returns in the original sample from a distribution $F ( \cdot )$ , and $T _ { n } = T _ { n } ( r _ { 1 } , r _ { 2 } , \cdot \cdot \cdot , r _ { n } )$ is a statistic that estimates a parameter $\theta$ . The sampling distribution of $T _ { n }$ depends on $F ( \cdot )$ . The core idea of the bootstrap is to estimate the cumulative distribution function (cdf) $F ( \cdot )$ using the empirical cdf $F _ { n } ( \cdot )$ . The empirical cdf $F _ { n } ( \cdot )$ is the nonparametric maximum likelihood estimate (MLE) of the cdf $F ( \cdot )$ . Bootstrapping based on $F _ { n } ( \cdot )$ is known as the nonparametric bootstrap. We can draw samples from $F _ { n } ( \cdot )$ , which is equivalent to drawing independent and identically distributed (iid) samples from $\{ r _ { 1 } , r _ { 2 } , \cdots , r _ { n } \}$ . This process can be repeated as many times as needed.

# 5.1 Bootstrap Framework

Since $F ( \cdot )$ is unknown, the sampling distribution of $T _ { n }$ is also unknown. As a result, we cannot determine the variance of $T _ { n }$ , i.e., $\mathrm { V a r } ( T _ { n } )$ , nor the confidence interval of $T _ { n }$ , i.e., $\mathrm { C I } ( T _ { n } )$ . Resample $\pmb { r } _ { n b } ^ { * } = \{ r _ { 1 } ^ { * } , r _ { 2 } ^ { * } , \cdots , r _ { n } ^ { * } \} _ { b }$ from $\boldsymbol { r } _ { n }$ using SRSWR scheme; $b = 1 , 2 , \cdots , B$ . For each resample $b$ , we can compute $T _ { n b } ^ { * }$ ; $b = 1 , 2 , \cdots , B$ . Then we can compute:

$$
\begin{array} { r c l } { { \displaystyle \bar { T } _ { n } ^ { B } } } & { { = } } & { { \displaystyle \frac { 1 } { B } \sum _ { b = 1 } ^ { B } T _ { n b } ^ { * } ; ~ V a r ( T _ { n } ) ^ { B } = \frac { 1 } { B } \sum _ { b = 1 } ^ { B } ( T _ { n b } ^ { * } - \bar { T } _ { n } ^ { B } ) ^ { 2 } } } \\ { { { \cal C I } ( T _ { n } ) ^ { B } } } & { { = } } & { { \{ T _ { n } + G _ { B } ^ { - 1 } ( \alpha / 2 ) \sqrt { V a r ( T _ { n } ) ^ { B } } , } } \\ { { } } & { { } } & { { T _ { n } + G _ { B } ^ { - 1 } ( 1 - \alpha / 2 ) \sqrt { V a r ( T _ { n } ) ^ { B } } \} , } } \end{array}
$$

where $\begin{array} { r } { \frac { T _ { n b } ^ { * } - T _ { n } } { \sqrt { V a r ( T _ { n } ) ^ { B } } } \sim G _ { B } } \end{array}$ . Due to SLLN, one can show, as $B \longrightarrow \infty$

$$
\begin{array} { r c l } { { \bar { T } _ { n } ^ { B } } } & { { \longrightarrow } } & { { T _ { n } \mathrm { a l m o s t s u r e l y } ; } } \\ { { V a r ( T _ { n } ) ^ { B } } } & { { \longrightarrow } } & { { V a r ( T _ { n } ) \mathrm { a l m o s t s u r e l y } } } \\ { { C I ( T _ { n } ) ^ { B } } } & { { \longrightarrow } } & { { C I ( T _ { n } ) \mathrm { a l m o s t s u r e l y } , } } \end{array}
$$

$$
\begin{array} { r l r } { G ^ { B } } & { { } \longrightarrow } & { F _ { T _ { n } } ( \cdot ) ~ \mathrm { i n } ~ \mathrm { l a w } . } \end{array}
$$

# 5.2 Residual Bootstrap Regression for CAPM

Consider the capital asset pricing model

$$
\pmb { r } _ { n } = \pmb { X } _ { n \times p } \pmb { \beta } _ { p } + \pmb { \epsilon } _ { n } ,
$$

where $\mathbb { E } ( \epsilon ) = 0$ , $\mathbb { V } a r ( \epsilon ) = \sigma ^ { 2 } I _ { n }$ , and $\epsilon \overset { i i d } { \sim } F ( \cdot )$ , $F ( \cdot )$ is unknown cdf. The OLS estimatoris ${ \hat { \boldsymbol { \beta } } } _ { n } =$ $( X ^ { T } X ) ^ { - 1 } X ^ { T } r$ ; and $\mathbb { V } a r ( \hat { \beta } _ { n } ) = \sigma ^ { 2 } ( X ^ { T } X ) ^ { - 1 }$ . The residuals are $\epsilon = r - X \hat { \beta } _ { n }$ or $\epsilon _ { i } = y _ { i } - x _ { i } ^ { T } \hat { \beta } _ { n } , i =$ $1 , 2 , \cdots , n$ . Suppose $F _ { n } ( \cdot )$ is the empirical cdf of $\epsilon$ . $\epsilon _ { b } ^ { * } \stackrel { i i d } { \sim } F _ { n }$ (i.e., $\epsilon _ { b } ^ { * }$ is resampled from $\epsilon$ using SRSWR), $b = 1 , 2 , \cdots , B$ . We calculate,

$$
\boldsymbol { r } _ { b } ^ { * } = \boldsymbol { X } \hat { \boldsymbol { \beta } } _ { n } + \epsilon _ { b } ^ { * } ,
$$

then estimate resample coefficients ${ \hat { \boldsymbol { \beta } } } _ { n : b } ^ { * }$ as

$$
\begin{array} { r c l } { \hat { \boldsymbol { \beta } } _ { n : b } ^ { * } } & { = } & { ( \boldsymbol { X } ^ { T } \boldsymbol { X } ) ^ { - 1 } \boldsymbol { X } ^ { T } \boldsymbol { r } _ { b } ^ { * } } \\ & { = } & { \hat { \boldsymbol { \beta } } _ { n } + ( \boldsymbol { X } ^ { T } \boldsymbol { X } ) ^ { - 1 } \boldsymbol { X } ^ { T } \boldsymbol { \epsilon } _ { b } ^ { * } } \\ { \mathrm { w h e r e ~ } \mathbb { E } ( \hat { \boldsymbol { \beta } } _ { n : b } ^ { * } ) } & { = } & { \hat { \boldsymbol { \beta } } _ { n } . } \end{array}
$$

Then we have the bootstrap estimate, $\begin{array} { r } { \bar { \boldsymbol { \beta } } _ { B } = \frac { 1 } { B } \sum _ { b = 1 } ^ { B } \hat { \boldsymbol { \beta } } _ { b } ^ { * } } \end{array}$ . and the bootstrap variance is

$$
\mathbb { V } a r ( \bar { \beta } _ { B } ) = \frac { 1 } { B } \sum _ { b = 1 } ^ { B } ( \hat { \boldsymbol { \beta } } _ { b } ^ { * } - \bar { \boldsymbol { \beta } } _ { B } ) ^ { 2 } .
$$

# 5.3 Paired Bootstrap Regression

We consider the model

$$
\pmb { r } _ { n } = \pmb { X } _ { n \times p } \pmb { \beta } _ { p } + \pmb { \epsilon } _ { n } ,
$$

where $\mathbb { E } ( \epsilon ) = 0$ , $\mathbb { V } a r ( \pmb { \epsilon } ) = \pmb { \Sigma }$ , and $( r _ { i } , \pmb { x } _ { i } ) \overset { i i d } { \sim } F ( \cdot )$ , where $F ( \cdot )$ is an unknown cdf. Suppose $\{ ( r _ { i } ^ { * } , \pmb { x } _ { i } ^ { * } ) , i =$ $1 , 2 , . . . n \} _ { b } = { \mathcal D } _ { b }$ are iid samples from empirical $F _ { n } ( \cdot )$ , where $b = 1 , 2 , \cdots , B$ . The estimates of $\beta$ from $b ^ { t h }$ resample is,

$$
\boldsymbol { \hat { \beta } } _ { b } ^ { \ast } = ( \boldsymbol { X } _ { b } ^ { \ast T } \boldsymbol { X } _ { b } ^ { \ast } ) ^ { - 1 } \boldsymbol { X } _ { b } ^ { \ast T } \boldsymbol { r } _ { b } ^ { \ast } .
$$

The bootstrap estimate is $\begin{array} { r } { \bar { \boldsymbol { \beta } } _ { B } = \frac { 1 } { B } \sum _ { b = 1 } ^ { B } \hat { \boldsymbol { \beta } } _ { b } ^ { * } } \end{array}$ , and the bootstrap variance is

$$
\mathbb { V } a r ( \bar { \beta } _ { B } ) = \frac { 1 } { B } \sum _ { b = 1 } ^ { B } ( \hat { \boldsymbol { \beta } } _ { b } ^ { * } - \bar { \boldsymbol { \beta } } _ { B } ) ^ { 2 } .
$$

Remark 5.1. If the residuals are heteroscedastic, the paired bootstrap remains a consistent estimator. However, when the residuals are heteroscedastic, the residual bootstrap is not a consistent estimator.

# 5.4 CAPM with Bootsrap Statistics usin R

Now, we will demonstrate the Capital Asset Pricing Model using the nonparametric bootstrap regression technique in R. First, we will download the data for Reliance and Nifty 50 from Yahoo. Next, we will calculate the log-returns and then the risk-premium. After that, we will fit the CAPM using the OLS method and obtain the residuals.

library(tseries)   
start_date<-"2024-01-01"   
end_date<-"2024-06-30"   
rel<-get.hist.quote(instrument $=$ "RELIANCE.NS",start $=$ start_date ,end $\underline { { \underline { { \mathbf { \Pi } } } } }$ end_date,quote $=$ "AdjClose" ,provider $=$ "yahoo",quiet $=$ TRUE)   
nifty<-get.hist.quote(instrument $=$ "^NSEI",start $=$ start_date ,end $\underline { { \underline { { \mathbf { \Pi } } } } } =$ end_date,quote $=$ "AdjClose" ,provider $=$ "yahoo",quiet $=$ TRUE)   
data <-merge(nifty,rel)   
rt<-diff(log(data))   
risk_free_rate<-0.06/252   
## risk premium   
rt<-rt-risk_free_rate   
## Fit CAPM using OLS   
CAPM $< -$ lm(Adjusted.rel\~Adjusted.nifty,data ${ . } = { }$ rt)   
## Extract residual and fitted values   
resid <- CAPM\$residuals   
y_hat <- CAPM\$fitted.values   
set.seed(6587)   
rt1<-data.frame(rt)   
n <- nrow(rt1)   
$\mathtt { B < - 1 0 0 0 }$   
beta_star<-matrix(NA,nrow $\mathrm { \Phi = B }$ ,ncol $\ c = \ 2$ )   
colnames(beta_star) $< - \subset$ (’alpha’,’beta’)   
R.squred_star.pair<-rep(NA,B)   
for(b in 1:B){ id_star<-sample(1:n,n,replace $=$ TRUE) rt_star<-rt1[id_star,] CAPM_star<-lm(Adjusted.rel $\sim$ Adjusted.nifty,data ${ . } = { }$ rt_star) sum_star<-summary(CAPM_star) beta_star[b,] $< -$ coef(CAPM_star) R.squred_star.pair[b] $< -$ sum_star\$adj.r.squared   
}   
sum_boot <-cbind(apply(beta_star,2,mean) ,apply(beta_star,2,sd) ,apply(beta_star,2,quantile,probs $= 0$ .025) ,apply(beta_star,2,quantile,probs $= 0$ .975))   
colnames(sum_boot)<-c(’Estimate’,’Std.Error’,’2.5%’,’97.5%’)   
cat(’OLS Estimates of alpha and beta’)   
ols_estimates<-coefficients(summary(CAPM))   
rownames(ols_estimates)<-c(’alpha’,’beta’)   
round(ols_estimates,4)   
cat(’Paired Bootstrap Estimates of alpha and beta’)   
round(sum_boot,4)   
Paired Bootstrap Estimates of alpha and beta Estimate Std.Error 2.5% 97.5%   
alpha 0.0005 0.0010 -0.0012 0.0024   
beta 1.2439 0.1309 0.9804 1.5023   
par(mfrow $= _ { \mathsf { C } }$ (1,2))   
hist(beta_star[,’alpha’],main $\begin{array} { r l } { \mathrm { ~  ~ \omega ~ } } & { { } = \mathrm { ~  ~ \omega ~ } \cdot \mathrm { ~  ~ \omega ~ } \cdot \mathrm { ~  ~ \omega ~ } } \end{array}$ , $\scriptstyle \mathbf { C } \supset \mathbf { 1 } = \mathbf { \prime }$ skyblue’ ,freq $=$ FALSE,xlab $=$ expression(alpha),nclass = 20)   
lines(density(beta_star[,’alpha’]), $\scriptstyle \mathbf { C } \supset \mathbf { 1 } = \mathbf { \prime }$ purple’,lwd $^ { = 2 }$ )   
hist(beta_star[,’beta’],main $\begin{array} { r l } { \mathrm { ~  ~ \omega ~ } } & { { } = \mathrm { ~  ~ \omega ~ } \cdot \mathrm { ~  ~ \omega ~ } \cdot \mathrm { ~  ~ \omega ~ } } \end{array}$ , $\scriptstyle \mathbf { C } \supset \bot = ^ { \prime }$ skyblue’,freq $=$ FALSE ,xlab $=$ expression(beta),nclass $=$ 20)   
lines(density(beta_star[,’beta’]), $\scriptstyle \mathbf { C } \supset \bot = ^ { \prime }$ purple’, $\bot \mathrm { w d } = 2$ )

![](images/e82fe7e34655a57a55db421a02624d6aa5bfc5e505b4eceb847f525f7a347b2f.jpg)  
Figure 3: Bootstrap histogram of $\alpha$ and $\beta$ of CAPM.

# Residual Bootstrap Regression

set.seed(6587)

ols_resid $< -$ CAPM\$residuals   
ols_pred <- CAPM\$fitted.values   
Adjusted.nifty $< -$ rt\$Adjusted.nifty   
$\mathtt { B < - 1 0 0 0 }$   
beta_star2<-matrix(NA,nrow $\mathrm { = B }$ ,ncol $\ c = \ 2$ )   
colnames(beta_star2) ${ < - } \subset$ (’alpha’,’beta’)   
R.squred_star.resid<-rep(NA,B)   
n <- nrow(rt1)   
for(b in 1:B){ id_star<-sort(sample(1:n,n,replace $=$ TRUE)) resid_star<-ols_resid[id_star] pred_star $< -$ ols_pred+resid_star CAPM_star<-lm(pred_star\~Adjusted.nifty) sum_star<-summary(CAPM_star) beta_star2[b,] $< -$ coef(CAPM_star) R.squred_star.resid[b] <- sum_star\$adj.r.squared   
}

# Summary of Residual Bootstrap Regression for CAPM

sum_boot2 <-cbind(apply(beta_star2,2,mean) ,apply(beta_star2,2,sd) ,apply(beta_star2,2,quantile,probs $= 0$ .025) ,apply(beta_star2,2,quantile,probs $= 0$ .975))   
colnames(sum_boot2) $< - \subset$ (’Estimate’,’Std.Error’,’2.5%’,’97.5%’)   
ols_estimates<-coefficients(summary(CAPM))

# OLS Estimates of $\alpha$ and $\beta$

$>$ rownames(ols_estimates)<-c(’alpha’,’beta’) $>$ round(ols_estimates,4) Estimate Std. Error t value Pr(>|t|) alpha 0.0006 0.0010 0.6285 0.5309 beta 1.2441 0.0981 12.6794 0.0000

# Residual Bootstrap Estimates of $\alpha$ and $\beta$

$>$ round(sum_boot2,4) Estimate Std.Error 2.5% 97.5% alpha 0.0006 0.0010 -0.0012 0.0026 beta 1.2462 0.0838 1.0887 1.4216

# Paired Bootstrap Estimates of $\alpha$ and $\beta$

> round(sum_boot,4)Estimate Std.Error 2.5% 97.5%alpha 0.0006 0.0010 -0.0012 0.0026beta 1.2379 0.1283 0.9935 1.5086

# 6 Empirical Risk Analysis of Passive Investment

# 6.1 Philosophy of Passive Investment

The philosophy of passive investment strategy yields from the ‘efficient market hypothesis.’ The logic is that since the market is efficient, it does not make sense that anyone will be able to beat the market consistently for a long time. Therefore, investing in funds that mimic the market only makes sense. It resulted in the popularisation of the exchange-traded fund (ETF), where the fund invests by mimicking the index weights. However, those weights are based on market capitalisation and are not necessarily optimal for investors. What kind of passive investment strategy would be better? In Result (4.2), we demonstrated that for a large portfolio with a fixed $P$ number of assets, an equal weight portfolio would yield minimum idiosyncratic risk, while we have no control over the systematic risk due to market movement.

# 6.2 Comparing of Two Passive Investment Strategies

In Section (2), we demonstrated that the Indian stock market based on the Nifty50 index is not efficient. So it raises the question how it affects the risk profile of the passive strategy (i.e., portfolio weight). To check this question, we decided to consider three different portfolios. First, we consider the passive investment portfolio with Nifty weights as the portfolio weights. Second, we consider Markowiz’s portfolio weights optimised with regularised covariance matrix. Third, we consider the equal weight as discussed in the Result (4.2), an alternate passive investment strategy.

<table><tr><td></td><td>Portfolio with Nifty Weights</td><td>Portfolio with Markowitz Weights</td><td>Portfolio with Equal Weights</td></tr><tr><td>Before the War</td><td>11.35</td><td>8.30</td><td>9.03</td></tr><tr><td>During the War</td><td>13.66</td><td>11.41</td><td>11.30</td></tr></table>

Table 1: Portfolio Volatility before the Russia-Ukraine war and during the war. Portfolio with Nifty weights have uniformly higher volatility than other portfolios, before and during the war. However, the volatility risk profile is portfolio with equal weights are that of similar to portfolio with Markowitz’s optimal weights.

Out of the three portfolios we considered here, the passive investment strategy with Nifty weights remained the same before and during the war. Similarly, the equal portfolio weights remained the same before and during the war. We use the before-war returns for Markowiz’s portfolio weights to estimate the weights using Markowitz’s optimisation with regularised covariance. Then we use the same weights to calculate the portfolio volatility during the war.

In Table (1), we present the portfolio Volatility before the Russia-Ukraine war and during the war. The portfolio with Nifty weights has uniformly higher volatility than other portfolios before and during the war. However, the volatility risk profile of a portfolio with equal weights is similar to a portfolio with Markowitz’s optimal weights because Markowitz’s weights are close to equal weights. The Result (3.2) states that idiosyncratic risk would be minimum for an equal weights portfolio. Therefore if we really want to have a passive investment strategy, we should try to implement equal-weight portfolios than Nifty weights. As Nifty weights are based on market capitalisation and not close to equal weights, a portfolio with Nifty 50 weights would contain a significant idiosyncratic risk than an equal weights portfolio.

![](images/89101003a1d897cf67cfca7139bdc752db78115044164fbbf723ff4c8932b0c6.jpg)  
Figure 4: The weights of Nifty 50 are much more skewed than Markowitz’s optimised weights. The horizontal red-dash line represents the equal-weight portfolio. Markowitz’s optimised weights are comparatively less deviate from equal weights.

<table><tr><td></td><td>Portfolio with Nifty Weights</td><td>Portfolio with Equal Weights VaR (ES)</td><td>Portfolio with Markowitz Weights</td></tr><tr><td>Before the War</td><td>VaR (ES) -5.75 (-6.07)</td><td>-5.53 (-5.84)</td><td>VaR (ES) -3.74 (-3.85)</td></tr><tr><td>During the War</td><td>-6.70 (-7.03)</td><td>-6.11 (-6.23)</td><td>-6.34 (-6.62)</td></tr></table>

Table 2: Portfolio VaR and Expected Shortfall (ES) before the Russia-Ukraine war and during the war. The numbers inside the parenthesis are ES and outside are VaR. The Portfolio with Nifty weights have uniformly higher VaR and ES than other portfolios, before and during the war.

# 7 Conclusion

In this work, we present that if we want to be passive investors, we should follow an equal-weight portfolio strategy instead of investing in Exchange Traded Fund like NETF, which mimics the Nifty50. We also present an analysis of how portfolios perform to idiosyncratic events like the Russian invasion of Ukraine. We found that the equal weight portfolio has a uniformly lower risk than the Nifty 50 portfolio before and during the Russia-Ukraine war. We also showed in Results (4.1,4.2) that if we push the maximum weights of the portfolio towards the equal weight portfolio, then the idiosyncratic risk of the portfolio is minimal. As a result, the equal-weight portfolio has a uniformly lower risk before and during the Russia-Ukraine war than the Nifty 50 portfolio.

# References

G. G. Baigent. X-sigma-rho and market efficiency. Journal of Economic and Financial Studies, 2, 2014.

Fischer Black. Capital market equilibrium with restricted borrowing. The Journal of Business, 45:444–455, 1972.

Sourish. Das and Dipak. K. Dey. On bayesian inference for generalized multivariate gamma distribution. Statistics and Probability Letters, 51:1492–1499, 2010.   
Sourish Das and Rituparna Sen. Sparse portfolio selection via bayesian multiple testing. Sankhya - B, Nov 2020.   
Sourish. Das, Aritra. Halder, and Dipak. K. Dey. Regularizing portfolio risk analysis: A bayesian approach. Methodology and Computing in Applied Probability, 19:865–889, 2017.   
David. A. Dickey and Wayne. A. Fuller. Distribution of the estimators for autoregressive time series with a unit root. Journal of the American Statistical Association, 74:427–431, 1979.   
National Stock Exchange. Netf: Tata nifty exchange traded fund. https://www.nseindia.com/ get-quotes/equity?symbol $=$ NETF, 2022. Accessed: 2022-11-18.   
O. Ledoit and M. Wolf. Improved estimation of the covariance matrix of stock returns with an appli- cation to portfolio selection. Journal of Empirical Finance, pages 603–621, 2003.   
G. M. Ljung and George. P. Box. On a measure of lack of fit in time series models. Biometrika, 65:297–303, 1978.   
Harry. Markowitz. Portfolio selection. Journal of Finance, pages 77–91, 1952.   
N. Matloff, editor. The Art of R Programming: A Tour of Statistical Software Design. The Journal of Portfolio Management, 2011. ISBN 9781593274108: 347.   
Jose. Menchero and Ben. Davis. Risk contribution is exposure times volatility times correlation: Decomposing risk using the x-sigma-rho formula. The Journal of Portfolio Management, 37: 97–106, 2011.   
S. S. Shapiro and M. B. WILK. An analysis of variance test for normality (complete samples). Biometrika, 52:591–611, 1965.   
William. F. Sharpe. Capital asset prices: A theory of market equilibrium under conditions of risk. Journal of Finance, 19:425–442, 1964.   
Steven E. Shreve, editor. Stochastic Calculus for Finance I The Binomial Asset Procing Model. Springer, 2004a. ISBN 13:987-0387-24968-1.   
Steven E. Shreve, editor. Stochastic Calculus for Finance II Continuous Time Model. Springer, 2004b. ISBN 978-0-387-40101-0.

# Appendix A: No Arbitrage: No Free Lunch

# Portfolio of Bonds and Shares

A bond is a debt security, earning a fixed rate of interest $r$ in each unit of time. If we make an investment $B _ { 0 }$ at time 0 in the bond is worth $B _ { 0 } ( 1 + r ) ^ { k }$ at time $k$ . If our interest earning is compounding over $k$ -period, then the bond will be worth $B _ { 0 } ( 1 + r ) ^ { k }$ at time $k$ . As bond can be bought or sold, as an investor we can invest or borrow at the rate of interest $r$ . We can trade shares of the stock of a specific company in the stock market. The price $P _ { k }$ at which one share of a stock can be traded is modeled as a stochastic process. Bond and stock are together known as securities (aka. primary securities.) We can consider our investment in house or real estate as primary security. However, the market for the real estate behaves very differently than the typical bond and stock market. The quantitative finance primarily focuses on the bond and stock market and anything related to that.

Suppose price of a stock is such that

$$
\mathbb { P } ( P _ { 1 } \geq P _ { 0 } ( 1 + r ) ) = 1 , \mathbb { P } ( P _ { 1 } > P _ { 0 } ( 1 + r ) ) > 0 .
$$

Then we can borrow an amount $P _ { 0 }$ and buy one share of the stock at time 0. At time 1, we can sell the stock at a price $P _ { 1 }$ , settle our debt by paying $P _ { 0 } e ^ { r }$ and our profit $P r o f i t = P _ { 1 } - P _ { 0 } ( 1 + r )$ is non-negative with probability one and is strictly positive with positive probability. It can be argued if such stock is available in the market and all information is available to everyone, then many investors like us would like to invest large amounts of money (by borrowing) into the stock; since there is nothing to lose and something to be gained. This will disturb the equilibrium and push the price of the stock (at time 0) up. Such opportunities are known as an arbitrage opportunity. In general, in a market consisting of several securities, an arbitrage opportunity is a strategy of buying and selling the securities without any investment, such that it leads to profit (strictly positive) with positive probability without any risk of a loss.

The price of the stock on the $k ^ { t h }$ day is denoted by $P _ { k }$ . The price $P _ { 0 }$ of the stock on day zero is assumed to be deterministic, $P _ { 0 } = p _ { 0 }$ . Also the face value of the bond is 1 on the morning of day zero. For $k \geq 0$ , let $\theta _ { k } ^ { S }$ denote the number of shares we decide to hold on the morning of $\bar { k } ^ { t h }$ day, before the market opens. Suppose $\theta _ { k } ^ { B }$ denote the number of bonds we choose to keep. If for $k \geq 1$ , $\theta _ { k } ^ { S } \ge \theta _ { k - 1 } ^ { S }$ , we buy $\theta _ { k } ^ { S } - \theta _ { k - 1 } ^ { S }$ shares and if $\theta _ { k } ^ { S } < \theta _ { k - 1 } ^ { S }$ , then we sell $\theta _ { k - 1 } ^ { S } - \theta _ { k } ^ { S }$ shares. We have same interpretation for bonds. In order to implement a strategy we may have to put in extra money on certain days while surplus on other days. However, we consider a special kind of strategy, known as self-financing strategies. These are trading strategies where there is no money put in and there is no surplus on any day except for the initial investment $x$ , where

$$
x = \theta _ { 0 } ^ { B } + \theta _ { 0 } ^ { S } { \cal P } _ { 0 } .
$$

Thus, on a given day, we only moves our money from shares to bonds or vice-versa. The shares and bonds held by us is known as our portfolio.

# Martingle and Arbitrage

Let $\mathcal { F } _ { i } = \sigma \{ P _ { j } : 0 \leq j \leq i \}$ is a finite $\sigma$ -field generated by $\{ P _ { 0 } , \ldots , P _ { i } \}$ . Suppose there exists a probability measure $\mathbb { Q }$ on $\mathcal { P }$ such that $\{ P _ { i } ( 1 + r ) ^ { i } , { \mathcal F } _ { i } \}$ is a $\mathbb { Q }$ -martingale and

$$
\mathbb { Q } ( p _ { 0 } , \dots , p _ { N } ) > 0 \quad \forall \ ( p _ { 0 } , \dots , p _ { N } ) \in { \mathcal { P } } .
$$

For any self-financing strategy $\eta$ , the worth of the portfolio on $k ^ { t h }$ day is

$$
V _ { k } ( \eta ) ( P _ { 0 } , \cdots , P _ { N } )
$$

is a $\mathbb { Q }$ -martingale, and hence

$$
\mathbb { E } ^ { \mathbb { Q } } [ V _ { k } ( P _ { 0 } , \dots , P _ { N } ) ( 1 + r ) ^ { - k } ] = V _ { 0 } ( \eta ) .
$$

Let $\eta$ be such that $V _ { 0 } ( \eta ) = 0$ , that is initial investment is zero, then (7.2) implies

$$
V _ { N } ( \eta ) ( p _ { 0 } , \ldots , p _ { N } ) = 0 \forall ( p _ { 0 } , \ldots , p _ { N } ) \in \mathcal { P } .
$$

Thus arbitrage opportunities, i.e.,

$$
V _ { N } ( \eta ) ( p _ { 0 } , \ldots , p _ { N } ) \geq 0 , { \mathrm { ~ f o r ~ a l l ~ } } ( p _ { 0 } , \ldots , p _ { N } ) \in { \mathcal { P } } ,
$$

and

$$
V _ { N } ( \eta ) ( p _ { 0 } ^ { * } , \ldots , p _ { N } ^ { * } ) > 0 , { \mathrm { ~ f o r ~ s o m e ~ } } ( p _ { 0 } ^ { * } , \ldots , p _ { N } ^ { * } ) \in { \mathcal { P } } .
$$

strategies satisfying (7.3) and (7.4), do not exist.

# Theorem 7.1. Fundamental Theorem of Asset Pricing

The following statements are equivalent.

1. No arbitrage.

2. There eqxists a probability measure $\mathbb { Q }$ on $\mathcal { P }$ such that $\{ P _ { i } ( 1 + r ) ^ { i } , { \mathcal F } _ { i } \}$ is a $\mathbb { Q }$ -martingale and

$$
\mathbb { Q } ( p _ { 0 } , \dots , p _ { N } ) > 0 \forall ( p _ { 0 } , \dots , p _ { N } ) \in \mathcal { P } .
$$

# Asset Pricing Model

So far we assumed that there is an underlying stochastic process for stock price movement. But no effort is rendered to model the stock price movement. We need a useful probability model to use as a computationally tractable approximation to theoretical models. The model should explain no-arbitrage pricing and its relation to risk-neutral pricing. Also, the model should incorporate the theory of conditional expectation and martingale theory which is the core of the risk-neutral pricing. To fulfill all these purposes the Asset Pricing Model, (APM) serves a good point to start. With this motive in mind, we present the geometric Brownian motion (GBM) model which is a slightly different from that usually found in practice.

We can model the price movement over time interval $[ 0 , t ]$ as a sequence of multiple binary steps. We can break the range into $n$ equal intervals per unit time. So over the range, the stock price will move through $n t$ binary steps and let the price follow one of the up or down branches each over each subinterval as presented in the figure (5). Note that we should choose $n$ and $t$ in such a way such that nt to be an integer.

# Assumption:

1. Each step is independent of previous one.   
2. Parameters $p , u$ and $d$ are same for each step.

Suppose $P _ { 0 }$ is the initial stock price. At each time step, the stock price either goes up by a factor of $u$ with probability $p$ or down by a factor of $d$ with probability $1 - p$ . We define the following indicator variable as

$$
X _ { i } = { \left\{ \begin{array} { l l } { 1 } & { { \mathrm { ~ i f ~ } } P _ { i } = P _ { i - 1 } u , } \\ { 0 } & { { \mathrm { ~ i f ~ } } P _ { i } = P _ { i - 1 } d . } \end{array} \right. }
$$

The fundamental asset pricing theorem (7.1) states that to have the no arbitrage opportunity, there must be a probability of these outcomes that satisfy the $\mathbb { Q }$ -martingale measure conditions at each step. That is

$$
\mathbb { E } ( P _ { i } | P _ { i - 1 } ) = P _ { i - 1 } ( 1 + \frac { r } { n } ) ,
$$

where

$$
p P _ { i - 1 } u + ( 1 - p ) P _ { i - 1 } d = P _ { i - 1 } ( 1 + \frac { r } { n } ) ,
$$

which implies

$$
{ \hat { p } } = { \frac { ( 1 + { \frac { r } { n } } ) - d } { u - d } } .
$$

Note that $\hat { p }$ is known as the risk-neutral probability. The stock price at time $t$ is still a function of initial price $P _ { 0 }$ and result of $n t$ binary steps. Suppose

where

$$
n t = U _ { n t } + D _ { n t } .
$$

The random walk $R _ { n t }$ is the difference between the number of up and down moves, i.e.,

$$
R _ { n t } = U _ { n t } - D _ { n t } .
$$

The three steps model is graphically presented in Figure (5).

![](images/3b4a251f4f9c2dfb9fcbbc17dcc760c086d03bd19d5a29da3d9f9958ef87098e.jpg)  
Figure 5: Three Period Binomial Model

# Geometric Brownian Motion

Let $\sigma > 0$ is a known constant and $r \geq 0$ is the interest rate. In this model the interest rate per period is $\textstyle { \frac { r } { n } }$ , the up factor is $u _ { n } = \exp \{ \sigma / \sqrt { n } \}$ and the down factor is $d _ { n } = \exp \{ - \sigma / \sqrt { n } \}$ . The risk-renutral probability is then

$$
\hat { p } _ { n } = \frac { 1 + \frac { r } { n } - \exp \{ - \sigma / \sqrt { n } \} } { \exp \{ \sigma / \sqrt { n } \} - \exp \{ - \sigma / \sqrt { n } \} } .
$$

Let $t$ be an arbitrary positive rational rational number, and for each positive integer $n$ , for which $n t$ is an integer, define

$$
R _ { n t : n } = \sum _ { i = 1 } ^ { n t } X _ { k : n } ,
$$

where $X _ { 1 : n } , X _ { 2 : n } , \ldots , X _ { n : n }$ are independent, identically distributed random variables with

$$
\begin{array} { r } { \tilde { \mathbb { P } } ( X _ { n : k } = 1 ) = \boldsymbol { \hat { p } } _ { n } , \quad \tilde { \mathbb { P } } ( X _ { n : k } = - 1 ) = 1 - \boldsymbol { \hat { p } } _ { n } , k = 1 , 2 , \ldots , n . } \end{array}
$$

The stock price at time $t$ in this model is

$$
\begin{array} { l l l } { { P _ { n } ( t ) } } & { { = } } & { { { \displaystyle P _ { 0 } u _ { n } ^ { \frac { 1 } { 2 } ( n t + R _ { n t : n } ) } d _ { n } ^ { \frac { 1 } { 2 } ( n t - R _ { n t : n } ) } } } } \\ { { \ } } & { { = } } & { { { \displaystyle P _ { 0 } \exp \left\{ \frac { \sigma } { 2 \sqrt { n } } ( n t + R _ { n t : n } ) \right\} \exp \bigg \{ - \frac { \sigma } { 2 \sqrt { n } } ( n t - R _ { n t : n } ) \bigg \} } } } \\ { { \ } } & { { = } } & { { { \displaystyle P _ { 0 } \exp \left\{ \frac { \sigma } { \sqrt { n } } R _ { n t : n } \right\} } . } } \end{array}
$$

In the Chapter 3 of Shreve [2004b], it is presented that as

$$
n  \infty , \frac { \sigma } { \sqrt { n } } R _ { n t : n } \xrightarrow { \mathcal { L } } N \big ( ( r - \sigma ^ { 2 } / 2 ) t , \sigma ^ { 2 } t \big ) .
$$

Hence the following theorem can be presented as follows.

Theorem 7.2. As $n \to \infty$ , the distribution of $P _ { n } ( t )$ in equation 7.5 converges to the distribution of

$$
P _ { t } = P _ { 0 } \exp \Bigg \{ \Bigg ( r - \frac { 1 } { 2 } \sigma ^ { 2 } \Bigg ) t + \sigma W _ { t } \Bigg \} ,
$$

where $W _ { t }$ is standard normal variable with $o$ mean and variance $t$ .

The stochastic process $P = \{ P _ { t } : t \geq 0 \}$ is a Geometric Brownian Motion (GBM) with drift parameter $\mu$ and volatility parameters $\sigma$ , where

$$
P _ { t } = P _ { 0 } \exp \Bigg \{ \Bigg ( \mu - \frac { \sigma ^ { 2 } } { 2 } \Bigg ) t + \sigma W _ { t } \Bigg \} ,
$$

where $W _ { t } \sim N ( 0 , t ^ { 2 } )$ is standard Brownian motion (BM).

# Geometric Brownian Motion under Risk-Neutral Measure

If we compare the equation (7.6) and (7.7), it is clear that under risk-neutral probability measure, if stock price follow the GBM then the drift parameter is the risk-free interest rate $r$ . That is

$$
P _ { t } = P _ { 0 } \exp \Bigg \{ \Bigg ( r - \frac { \sigma ^ { 2 } } { 2 } \Bigg ) t + \sigma W _ { t } \Bigg \} .
$$

# Exercise

1. How does the concept of the time value of money affect investment decisions, and what are the key differences between the present value (PV) and future value (FV) of a cash flow?

2. You are expected to receive $\yen 10,00,000$ five years from now. If the annual discount rate is $6 \%$ , what is the present value of this future amount?

3. Given the following monthly closing prices of a stock for the first quarter of the year, calculate the quarterly return using the log-return method:

(a) January: |120   
(b) February: |130   
(c) March: |125

4. Explain the Efficient Market Hypothesis (EMH) and discuss its three forms. How does each form of EMH impact an investor’s ability to achieve above-average returns?

5. How does the Random Walk Hypothesis relate to the Efficient Market Hypothesis, and what are the implications of the Random Walk Hypothesis for technical analysis and investment strategies?

6. What statistical tests can be used to evaluate the Random Walk Hypothesis in financial markets, and how do these tests determine if a time series of stock prices follows a random walk?

7. Explain the Capital Asset Pricing Model (CAPM) and its key assumptions. How is the expected return of a security calculated using CAPM, and what is the significance of the $\alpha$ and $\beta$ coefficient in this model?   
8. Explain the concepts of portfolio risk and portfolio volatility. How are these measures calculated? Additionally, define Value at Risk (VaR) and describe its significance in portfolio management. How can VaR be calculated for a given portfolio?   
9. Explain the concepts of Marginal Contribution to Total Risk (MCTR) and Conditional Contribution to Total Risk (CCTR) in portfolio management. How are these metrics used to assess the risk contributions of individual assets within a portfolio? Provide the formulas for calculating MCTR and CCTR.   
10. Why is bootstrap statistics important in conducting risk analysis? Explain the advantages of using the bootstrap method over traditional parametric methods in estimating the risk measures for a portfolio.   
11. What is the difference between residual and paired bootstrap regression?