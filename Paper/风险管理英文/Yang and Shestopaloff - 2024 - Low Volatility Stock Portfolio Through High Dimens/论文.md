# Low Volatility Stock Portfolio Through High Dimensional Bayesian Cointegration

Parley R Yang ry266@cam.ac.uk Faculty of Mathematics, University of Cambridge Cambridge, UK

Alexander Y Shestopaloff School of Mathematical Sciences, Queen Mary University of London London, UK

# Abstract

We employ a Bayesian modelling technique for high dimensional cointegration estimation to construct low volatility portfolios from a large number of stocks. The proposed Bayesian framework effectively identifies sparse and important cointegration relationships amongst large baskets of stocks across various asset spaces, resulting in portfolios with reduced volatility. Such cointegration relationships persist well over the out-of-sample testing time, providing practical benefits in portfolio construction and optimization. Further studies on drawdown and volatility minimization also highlight the benefits of including cointegrated portfolios as risk management instruments.

![](images/2bb724cbe52c452f07acb941a7cea98b159942a3a84cd397fd283645ab4e9be6.jpg)  
Figure 1. Index (black), random portfolios (grey), and cointegrated portfolio returns

# 1 Introduction

# 1.1 Motivation

Stationarity of a time series is defined as its first moment and auto-covariance being finite and invariant across time. Stock prices typically do not satisfy this assumption, though their lag-difference or log lag-difference does (such as in models assuming Brownian or Geometric Brownian motion). Cointegration, in this context, concerns with finding portfolios of stocks such that the portfolios can be stationary. This is beneficial for volatility reduction, as well as other portfolio applications such as drawdown and Sharpe ratio improvement.

More technically, let $Y ( i , t )$ denote the price of stock $i$ at the end of day ??. A vector of $\boldsymbol { p }$ stock prices at $t$ is denoted as $Y ( \cdot , t ) \in \mathbb { R } ^ { p }$ , abbreviated as $Y _ { t }$ , while a time series over $T$ days of a particular stock $i$ is denoted as $Y ( i , \cdot ) \in \mathbb { R } ^ { T }$ . We write $Y \sim I ( 1 )$ if it is non-stationary, but the time series $\Delta Y$ defined by $\Delta Y ( i , t ) : = Y ( i , t ) - Y ( i , t - 1 ) \ \forall i , t$ is stationary, i.e. $\Delta Y \sim I ( 0 )$ .

Now, cointegration in the setting of portfolio management concerns finding the dimension $r$ and linearly independent vectors $\alpha _ { 1 } , . . . , \alpha _ { r }$ such that $\alpha _ { i } Y \sim I ( 0 ) \forall i \in [ r ]$ . High dimensionality acts as a barrier to classical methodologies as $\boldsymbol { p }$ can be large, coupled with a reasonably sized $r$ to be determined. In most cases where $r < p$ , a random non-zero vector $\theta \in \mathbb { R } ^ { p }$ leading to the combination $\theta Y$ would stay non-stationary due to $\theta$ not being in the span of the cointegration vectors $\alpha _ { 1 } , . . . , \alpha _ { r }$ .

Additionally, high dimensionality is a clear motive for new models, and has direct implications to portfolio management, where feature dimension can be considered as the number of assets in an investable asset space. For instance, $\mathrm { S } \& \mathrm { P } \ 5 0 0$ index has 500 constituents stocks, with more than 400 of them being $I ( 1 )$ . If we were to run a Vector Autoregressive model (VAR), even with just 1 lag, the desired main estimation matrix would consist of $p ^ { 2 }$ undetermined parameters — in daily time series, if we were to match $T$ to $p ^ { 2 }$ this would equate to about 992 years of data.1 In this paper, we utilise a Bayesian model for sparsity to assist modelling high number of features with a low number of observations. This brings down the number of observations required to 2 years, while maintaining the capability to detect the underlining cointegration structure amongst circa 400 to 1000 stocks.

In Figure 1, we illustrate a typical scenario where stock indices and randomly generated unified long-short portfolios (randomly drawn $\theta$ where $| | \theta | | _ { 1 } ~ = ~ 1 )$ are plotted in black and grey. Cointegrated portfolios generated by the estimated $\alpha _ { 1 } , . . . , \alpha _ { r }$ using our Bayesian Models after unification2 are plotted in blue. The return of cointegrated portfolio in general has a clear low-volatility and a behaviour of more concentration around zero compared to the others, underpinning the topic of this paper’s discussion.

# 1.2 Literature Review

There are many existing literature in the fields we fit in, though the combination of them are rare. To start with, portfolio constructions and the relevant statistical studies have been covered in recent literature [2, 7, 10, 13]. In particular, Borragiero et al have introduced a ranking algorithm dealing with non-stationarity of stock data [2]. We face the same challenge in the empirics, however, the methodology we proposed relate more intimately to the notion of finding cointegration vectors amongst the non-stationary stock prices. In our application, we engage with the portfolio combination and volatility reduction, which are discussed under various Machine Learning framework such as in [10, 13]. Our application on the optimisation front inherits from the Markowitz portfolio [6], which have been reviewed extensively in conjunction with penalisation and other modern statistical techniques [3, 7]. There are another stream of literature on pairs trading [8], however, the cointegrated portfolio investigated here rests upon a higher dimensional search of assets — instead of finding two or three stocks on a pre-selected sub-basket.

In contrast to most of the above literature, the crux of our methodology focus heavily on the cointegration model and Bayesian posterior estimation. To pursue the technical review of literature, we start with the Vector Error Correction Model (VECM) model from a VAR(1), a simplification from [4]:

$$
\begin{array} { r } { \Delta Y _ { t } = \Pi Y _ { t - 1 } + \varepsilon _ { t } , \varepsilon _ { t } \sim \mathcal { N } ( 0 , \Sigma ) } \\ { r = r a n k ( \Pi ) \qquad } \end{array}
$$

The rank $r$ leads to the number of unknown cointegrated relationships, paving paths for constructing cointegrated portfolios in stock selection. As $\Pi \in \mathbb { R } ^ { p \times p }$ and that we face insufficient data for the estimation, we seek alternative ways to construct an estimator for $\Pi$ and $r$ . Liang and Schienle propose the penalised likelihood approach to regulate the sparsity and henceforth $r$ estimation [5]. Yang and Shestopaloff inherit their partial least square procedure in transforming a pre-estimation of $\Pi$ into an orthogonal decomposition, followed by Bayesian model on the sparse decomposition [12]. Further technical equipments for modelling include Spikeand-Slab Lasso (SSL), which are first presented by Rockova [9].

# 1.3 Contributions

There are two key contributions we make to both the academic audience and financial practitioners.

We summarise and extend the theoretical and algorithmic approach in finding high dimensional cointegration relationship (section 2) to the practical world, and find evidence of such cointegration in a rolling training and testing manner (section 3). Such cointegration relationships persist well over the out-of-sample testing time, providing practical benefits in portfolio construction and optimisation.

The portfolio optimisation brings benefit to practitioners’ risk management, as low volatility portfolio may be engaged in practitioners’ strategy combination — here we assume a simple passive index strategy as the benchmark and found combination with the cointegrated portfolio yield better Sharpe ratio out-of-sample (section 4). Further studies are conducted on drawdown and volatility minimisation, which also adds benefit for the inclusion of cointegrated portfolios as instruments for risk management (section 5).

# 1.4 Notations

Common notations on distributions are as follows: $\mathcal { N }$ indicates Gaussian distribution, $\mathcal { B }$ indicates Beta distribution, and $\boldsymbol { \mathcal U }$ indicates uniform distribution. For a vector $\theta \in \mathbb { R } ^ { p }$ and $a < b < p$ , $\theta _ { a : b } \in \mathbb { R } ^ { b - a + 1 }$ is the vector of entries $a$ to $b$ of $\theta$ . For a matrix $R \in \mathbb { R } ^ { p \times p }$ , the $j$ -th column is denoted as $R ( \cdot , j ) \in \mathbb { R } ^ { p \times 1 }$ . The set of positive integers up to $\boldsymbol { p }$ is denoted as $[ p ] : = \{ 1 , 2 , . . . , p \}$ . Other notations are introduced when they are mentioned.

# 2 High Dimensional Bayesian Cointegration Model

# 2.1 Decomposed VECM

The decomposed VECM could be summarised as follows. Let $Y _ { t } \in { \mathbb { R } } ^ { p }$ be the multivariate time series of interests. Denote the concatenated matrices $A : = [ \Delta Y _ { 1 } , . . . , \Delta Y _ { T } ] \in \mathbb { R } ^ { p \times T }$ and $B : = [ Y _ { 0 } , . . , Y _ { T - 1 } ] \in \mathbb { R } ^ { p \times T }$ , we implement the model as per Equation 1 and the following partial least square preestimation:

$$
\tilde { \Pi } = ( A B ^ { \intercal } ) ( B B ^ { \intercal } ) ^ { - 1 }
$$

Equation 1 is a typical VAR(1) model for the time series, whereas Equation 3 implements a pre-estimation on the unknown $\Pi \in \mathbb { R } ^ { p \times p }$ . Furthermore, we decompose $\tilde { \Pi }$ into

$$
\tilde { \Pi } = \tilde { R } ^ { \intercal } \tilde { S } ^ { \intercal }
$$

where $\tilde { R }$ is a $\mathrm { p }$ -dimensional upper triangular matrix and $\tilde { S }$ is an orthogonal matrix. Due to the orthogonality, $r a n k ( { \tilde { R } } ) =$ $r a n k ( \tilde { \Pi } )$ and more crucially, the rank would be the same as the number of non-zero rows of ${ \tilde { R } } ^ { \intercal }$ . Now, for an alternative estimate $\hat { R }$ , we may construct estimator

$$
\hat { \Pi } = \hat { R } ^ { \intercal } \tilde { S } ^ { \intercal }
$$

and henceforth $\hat { r } : = r a n k ( \hat { \Pi } ) = r a n k ( \hat { R } )$

The SSL distribution on decomposed VECM is, in this pursuance, tasked to use Bayesian method to detect the sparsity of matrix $R$ , for the purpose of estimating the true low rank.

# 2.2 SSL Distribution

We first introduce the Bayesian model behind the high dimensional VECM modelling — SSL distribution.

Let $X \in \mathbb { R }$ be a random variable. A Spike-and-Slab Lasso (SSL) prior distribution on $X$ is defined as

$$
\pi ( X | \gamma ) : = ( 1 - \gamma ) \psi _ { 0 } ( X ) + \gamma \psi _ { 1 } ( X ) , \gamma \in [ 0 , 1 ]
$$

$$
\psi _ { j } ( x ) : = { \frac { \lambda _ { j } } { 2 } } \exp ( - \lambda _ { j } | x | ) \forall j \in \{ 0 , 1 \} \ \mathrm { w i t h } \ \lambda _ { 0 } > \lambda _ { 1 } > 0
$$

We write $X | \gamma \sim \mathit { S S L } ( \gamma , \lambda _ { 0 } , \lambda _ { 1 } )$ . Let $X \in \mathbb { R } ^ { p }$ be a random variable. We write $X | \gamma \sim S S L ( \gamma , \lambda _ { 0 } , \lambda _ { 1 } )$ if for all entries $j \in$ $[ p ] , X _ { j } \vert \gamma \sim S S L ( \gamma , \lambda _ { 0 } , \lambda _ { 1 } )$ ??????

For intuition, $S S L ( \gamma , \lambda _ { 0 } , \lambda _ { 1 } )$ can be thought of as a mixing of two Laplace distributions, with $\psi _ { 0 }$ being spiky towards zero, while $\psi _ { 1 }$ being flatter.

# 2.3 SSL Distribution on Decomposed VECM

We first re-write the Equation 1 and Equation 5 with a simplification on $\Sigma = d i a g ( \sigma _ { 1 } ^ { 2 } , . . . , \sigma _ { \phi } ^ { 2 } ) \colon \forall t \in [ T ] , j \in [ p ] ,$

$$
\Delta Y _ { t , j } | Y _ { t - 1 } , R , \sigma _ { j } \sim \mathcal N ( R ( \cdot , j ) \tilde { S } ^ { \intercal } Y _ { t - 1 } , \sigma _ { j } ^ { 2 } )
$$

The variance is endowed with a Jeffery prior $\pi ( \sigma _ { j } ^ { 2 } ) \propto \sigma _ { j } ^ { - 2 } \forall j \in$ $[ p ]$

Now, for the modelling of $R$ , we do it column-wise, in particular, $\forall j \in [ p ]$

$$
\begin{array} { r l r } {  { R ( \cdot , j ) | \gamma _ { j } \sim S S L ( \gamma _ { j } , \lambda _ { 0 , j } , \lambda _ { 1 } ) , } } \\ & { } & { \gamma _ { j } | \theta _ { j } \sim B e r n o u l l i ( \theta _ { j } ) , ~ } \\ & { } & { \theta _ { j } \sim \mathcal { B } ( p - j + 1 , p ) ~ } \end{array}
$$

The essence of the model is that $R$ is modelled column-bycolumn with $\gamma _ { j }$ mixxing of different spikes $\lambda _ { 0 , j }$ . The proportion is governed by $\theta _ { j }$ , which in turn is drawn from a Beta distribution from levelling priors. We consider modelling parameter to be $R , \Sigma$ , and the estimator takes the standard maximum-a-posterior approach, namely

$$
( \hat { R } , \hat { \Sigma } ) = \underset { R , \Sigma } { \mathrm { a r g m a x } } \pi ( R , \Sigma | Y _ { 0 } , . . . , Y _ { T } )
$$

where $\pi ( \cdot , \cdot | Y _ { 0 } , . . . , Y _ { T } )$ is the posterior probability after conditional marginalisation on $\gamma _ { j }$ and likelihood updates as per Equation 8.

Subsequently, rank can be taken as $\hat { r } = r a n k ( \hat { R } )$

# 2.4 Algorithm: a brief overview

The solution to Equation 12 can be approximated through EM iterations column-wise (see [1, 12] for further discussions). We give a brief overview on the algorithms employed in this paper.

The algorithm 1 offers a pragmatic and efficient approach in dealing with finding suitable $\lambda _ { 0 , j }$ and the corresponding sparsity estimation. In particular, starting from line 2, a desired sparsity is declared, pushing $\lambda _ { 0 , j }$ to increase evenly3 until the mild condition of identifiability is reached — here we put $\begin{array} { r } { \hat { r } < \frac { T } { \sqrt { p } } } \end{array}$ to facilitate possible ranks of 10 to 20 when

Input: data $Y _ { 0 } , . . . , Y _ { T }$ , initialisers ?? (0) , ?? (0) , $( \sigma _ { j } ^ { ( 0 ) } ) _ { j \in [ \boldsymbol { p } ] }$ , and initialised SSL parameter $\lambda _ { 1 }$ , seed for Uniform draws

Output: estimated ranks $\hat { r }$ and matrix $\hat { R }$

1: Initialise estimate $\hat { r } = p$ , pre-process data (centralisa  
tion of $\Delta Y _ { t }$ and normalisation of $\tilde { S } ^ { \intercal } Y _ { t }$ ). Initialise $\lambda _ { 0 , j } =$   
$\lambda _ { 1 } \forall j \in [ p ]$   
2: while $\begin{array} { r } { \hat { r } \ge \frac { T } { \sqrt { p } } } \end{array}$ do   
3: Increase $\lambda _ { 0 , j }$ by the same amount for all $j$   
4: Compute new $\hat { R }$ and $\hat { r }$   
5: end while   
6: Initialise set $P = \{ j : \hat { R } ( \cdot , j ) \neq \mathbf { 0 } \}$ . Set seed for sampling.   
7: while $\hat { r }$ continues to decrease do   
8: Sample $j \sim \mathcal { U } ( P )$   
9: Increase $\lambda _ { 0 , j }$   
10: Compute new $\hat { R } ( \cdot , j )$   
11: if $\hat { \beta } _ { j } = \pmb { 0 } _ { p }$ then   
12: Update $P$ by $P \setminus \{ j \}$ and $\hat { r }$ by $\hat { r } - 1$   
13: end if   
14: end while   
15: return estimated rank $\hat { r }$ with the final matrix $\hat { R }$

$p \approx 1 0 0 0$ , with $T \approx 5 0 0$ . Followed by the initial search, a nonzero set of columns are established (in line 6), with further stochastic increase of $\lambda _ { 0 , j }$ being progressed. This makes the algorithm practical, and allows probabilistic inference — as each seed may result in different magnitude of $\lambda _ { 0 , j }$ . Practically, however, with small increment, the resulting $\hat { r }$ often concentrates to the same value.

# 3 Stationary Portfolios Through Cointegration

# 3.1 Portfolio construction upon cointegration

From section 2, we obtain $\hat { R }$ and therefore $\hat { \Pi }$ as mentioned in ???????????????? 5. Due to the orthogonality of $\tilde { S }$ , we would have $\hat { r }$ many linearly independent non-zero rows in $\hat { \Pi }$ . The idea then, is to use these rows to construct portfolios.

Let ${ \hat { \Pi } } _ { i }$ be a non-zero row of $\hat { \Pi }$ , define a stationary portfolio as $\begin{array} { r } { \alpha _ { i } : = \frac { \hat { \Pi } _ { i } } { | | \hat { \Pi } _ { i } | | _ { 1 } } } \end{array}$ . The interpretation of $\alpha _ { i }$ is the weight allocated to each stock in a long-short portfolio, summed to one. Additionally, $\hat { r }$ is interpreted as the number of stationary portfolios.

# 3.2 Data usage

In this paper, we present estimation results based on three asset spaces, on a rolling training $\&$ testing basis. Standard ADF tests are run to test whether the individual time series are $I ( 1 )$ .

The first asset space is S&P 500 index (SPY) constituents, with the index being considered as a benchmark for the US market. The list is obatined from the index-tracking ETF with ticker IVV, dated end of year 2023. There are 453 stocks which are tested to be $I ( 1 )$ . The second asset space is STOXX Europe 600 (SXXP) index constiuents, with the index being the benchmark for the European market. The list is obatined from the index-tracking ETF with ticker EXSA, also dated end of year 2023. There are 544 stocks which are tested to be $I ( 1 )$ . The third space is a combination of the first and the second, where the aforementioned US and European markets are combined together, resulting in 997 stocks.

<table><tr><td>End of training period</td><td>22-12</td><td>23-01</td><td>23-02</td><td>23-03</td><td>23-04</td><td>23-05</td><td>23-06</td><td>23-07</td><td>23-08</td><td>23-09</td><td>23-10</td><td>23-11</td><td>23-12</td><td>24-01</td></tr><tr><td>Number of Portfolios</td><td>9</td><td>23</td><td>6</td><td>19</td><td>6</td><td>17</td><td>12</td><td>8</td><td>10</td><td>9</td><td>11</td><td>11</td><td>14</td><td>13</td></tr><tr><td>Mean volatility</td><td>2.1</td><td>1.9</td><td>0.7</td><td>0.7</td><td>1.0</td><td>0.8</td><td>0.9</td><td>0.9</td><td>6.4</td><td>1.5</td><td>5.3</td><td>1.9</td><td>4.4</td><td>3.5</td></tr><tr><td>Median volatility</td><td>0.7</td><td>0.7</td><td>0.8</td><td>0.7</td><td>0.7</td><td>0.7</td><td>0.9</td><td>0.9</td><td>8.7</td><td>0.8</td><td>0.8</td><td>0.9</td><td>0.9</td><td>1.1</td></tr><tr><td>US Benchmark</td><td>21.2</td><td>21.2</td><td>21.2</td><td>21.3</td><td>21.3</td><td>21.3</td><td>21.4</td><td>21.3</td><td>21.4</td><td>21.4</td><td>21.4</td><td>21.4</td><td>21.1</td><td>20.8</td></tr><tr><td>European Benchmark</td><td>17.3</td><td>17.3</td><td>17.3</td><td>17.7</td><td>17.7</td><td>17.6</td><td>17.6</td><td>17.6</td><td>17.6</td><td>17.5</td><td>17.6</td><td>17.3</td><td>17.1</td><td>16.6</td></tr></table>

Table 1. Summary of portfolio volatility (in percentage) in the in-sample training periods

<table><tr><td>Testing period</td><td>23-01</td><td>23-02</td><td>23-03</td><td>23-04</td><td>23-05</td><td>23-06</td><td>23-07</td><td>23-08</td><td>23-09</td><td>23-10</td><td>23-11</td><td>23-12</td><td>24-01</td><td>24-02</td></tr><tr><td>Number of Portfolios</td><td>9</td><td>23</td><td>6</td><td>19</td><td>6</td><td>17</td><td>12</td><td>8</td><td>10</td><td>9</td><td>11</td><td>11</td><td>14</td><td>13</td></tr><tr><td>Mean volatility</td><td>2.3</td><td>2.0</td><td>1.0</td><td>1.1</td><td>1.3</td><td>0.9</td><td>1.4</td><td>1.0</td><td>3.7</td><td>1.7</td><td>4.0</td><td>1.8</td><td>3.0</td><td>2.6</td></tr><tr><td>Median volatility</td><td>1.2</td><td>1.3</td><td>1.0</td><td>1.1</td><td>1.2</td><td>0.9</td><td>1.4</td><td>1.0</td><td>4.7</td><td>1.3</td><td>1.1</td><td>1.1</td><td>1.2</td><td>1.3</td></tr><tr><td>US Benchmark</td><td>16.9</td><td>15.9</td><td>19.5</td><td>13.1</td><td>14.4</td><td>12.3</td><td>9.7</td><td>14.8</td><td>12.9</td><td>16.8</td><td>12.6</td><td>12.4</td><td>14.5</td><td>15.5</td></tr><tr><td>European Benchmark</td><td>11.0</td><td>11.7</td><td>21.8</td><td>8.7</td><td>13.0</td><td>10.9</td><td>17.1</td><td>10.3</td><td>11.6</td><td>13.8</td><td>11.2</td><td>6.1</td><td>11.2</td><td>8.6</td></tr></table>

Table 2. Summary of portfolio volatility (in percentage) in the out-of-sample testing periods

In terms of the treatment towards number of observations over time, we consider a rolling estimation approach for training and testing of the parameter estimation and induced portfolio. The first training period is from start of year 2021 to end of year 2022, spanning two years, resulting in 517 observations. The subsequent testing period is the January 2023. The proceeding training period is from the start of February 2021 to the end of January 2023, with testing period being the February 2023. The rolling proceeds for 14 times with the testing period ending at the end of February 2024.

To summarise, the feature dimension $\boldsymbol { p }$ is, respectively, 453, 544, and 997 in the first to third space, and $T$ is circa 510 to 520, depending on day-counts in the rolling periods.

# 3.3 Summary of Performance

The virtue of stationary portfolio is in volatility reduction. In Table 1, we summarise the estimation result in-sample. Over the three estimations, we found a range of 6 to 23 portfolios over time, with remarkably low volatility amongst them, demonstrated both in mean and median across the portfolios, and the reporting number can often be $9 0 \%$ or more lower compared to US or European benchmarks (the index volatility of SPY and SXXP respectively).

We proceed to test the same portfolio in testing period. In Table 2, we illustrate the testing result, where volatilities are computed upon the same portfolios as they were trained. For instance, the first set of 9 portfolios were trained based on data up to December 2022 (first column of Table 1) — these are then run in the testing period in January 2023. As reported, the low volatility is persistent throughout the out-of-sample testing period, which means the cointegrated portfolio tends to be robust in its low-volatility property in the testing period proceeding the in-sample training dataset.

# 3.4 Sample Portfolio Insight

![](images/2f2a2326ce905641797bf528bec2c105d5486d26729972b5a418826930037ace.jpg)  
Figure 2. Typical training and testing performance in portfolio returns

In Figure 2, we plot a typical estimation result to showcase the portfolio performance, where the rolling in-sample training period is July 2021 to June 2023, with testing being July 2023. 12 portfolios are generated from an estimated $\hat { r } = 4$ in the US market, $\hat { r } = 1$ in the European market, and $\hat { r } = 7$ in the joint market.

In-sample performance saw a clear low-volatility behaviour, which carries over to the testing period, admitting some shifting towards the end — typical behaviour of financial time series which have potential change-points in the un-seen periods.

Figure 3 is drawn for illustration of insight and portfolio analysis towards the aforementioned performance. Commonality may be observed amongst the estimated portfolios — for the joint portfolio, summed absolute percentages (grouped $\vert \alpha _ { i , j } \vert$ for some $j$ -th entry in $\alpha _ { i }$ ) tend to allocate a good portion to the US, with the joint portfolios focusing on the UK as the secondary choice in geography. Net percentages (grouped $\alpha _ { i , j }$ , hence some positive and some negative) depict differences across each portfolio, where some (mainly when the US market space is individually estimated) have net positive on Information Technology, whilst others are more settled.

![](images/5fa051db581d2e042326b6662f92463d90bcdc64c45145d0f275d4e0582accfb.jpg)  
Figure 3. Portfolio analytics: absolute percentage by location (upper) and net percentage by sectors (lower)

# 4 Combining Stationary Portfolio to Improve Sharpe Ratio

# 4.1 Motivation and Methodology

Let $\mu ( \alpha ) _ { t } , \sigma ( \alpha ) _ { t }$ be the daily return and (annualised) volatility of a portfolio $\alpha$ over day $t$ . The Sharpe ratio is defined as $\begin{array} { r } { S R ( \alpha ) = \mathbb { E } [ \frac { \mu ( \alpha ) _ { t } } { \sigma ( \alpha ) _ { t } } ] } \end{array}$ , and empirically the expectation is evaluated over the corresponding data. Given the stationary portfolio and its low volatility property we have demonstrated, it is tempting to pose the following question: can a combination of low volatility cointegrated portfolios and a passive strategy yield better Sharpe ratio? Intuitively, this may be seen as a classical notion of risk-return trade-off (back to the Markowitz portfolio selection), where Sharpe ratio may be improved by a reduction in return and volatility, due to the mixing of stationary portfolio.

To consider a combination portfolio, we first consider a simple setting to build up the financial intuition. Let vector $\alpha$ be such that $\alpha Y \sim I ( 0 )$ and $| | \alpha | | _ { 1 } = 1$ , which represents a typical cointegrated portfolio depicting stationarity. Let $\beta$ be a passive investment strategy (SPY or SXXP in this paper), it’s likely that $\beta Y \sim I ( 1 )$ and empirically yielding higher volatility. The combination strategy concerns a new portfolio $\theta \in ( 0 , 1 )$ where the resulting return takes the form of $\Psi = \theta \alpha Y + ( 1 - \theta ) \beta Y .$ Intuitively, while $\Psi$ could have lower return due to its allocation into cointegrated portfolio, it also could have lower volatility — perhaps lower to the extent that the Sharpe ratio becomes higher.

Certainly, as shown empirically, we can often find multiple stationary portfolios over the training data, so the practical optimisation is framed as follows. Given $r$ number of portfolios and $b$ number of baseline passive investment strategies4, written as matrices $\alpha : = ( \alpha _ { 1 } , . . . , \alpha _ { r } )$ and $\beta : = ( \beta _ { 1 } , . . . , \beta _ { b } )$ respectively, the combination portfolio takes the form of $\Psi \ = \ \theta _ { 1 : r } ^ { \intercal } \alpha Y + \theta _ { r + 1 : r + b } ^ { \intercal } \beta Y$ where $\theta ~ \in ~ [ - 1 , 1 ] ^ { r + b }$ such that $| | \theta | | _ { 1 } = 1$

In the training dataset indexed by $t \in$ train, we opt to optimise the following objective:

$$
{ \begin{array} { r l } & { { \mathrm { m a x i m i s e ~ } } S R ( \Psi ( \theta ) ) : = \mathbb { E } _ { t \in \mathsf { t r a i n } } \left[ { \frac { \mu ( \Psi ( \theta ) ) _ { t } } { \sigma ( \Psi ( \theta ) ) _ { t } } } \right] } \\ & { \qquad { \mathrm { s u b j e c t ~ t o ~ } } \theta \in \Theta } \end{array} }
$$

The maximiser $\theta ^ { * }$ is then taken to construct the optimised portfolio, proceeding into out-of-sample testing, where we aim to answer the initial question: whether $\theta ^ { * }$ , which combines the $r$ many cointegrated portfolios can improve Sharpe ratio.

The space $\Theta$ is set to such that some passive mixing are required — as far as this section is concerned, as a mainstream application. Specific definitions are set in each of the cases in the proceeding parts of this section.

# 4.2 US only

We first present the result where asset space is constrained in US. In this setting, $b = 1$ and $\Theta$ is set to such that $\textstyle \theta _ { r + 1 } \in { \bigl ( } { \frac { 1 } { 2 } } , 1 { \bigr ) }$ , meaning the combination portfolio must take at least half of the benchmark, which is SPY. Certainly, choosing how much the cointegrated portfolios are allocated to would be up to the optimisation objective.

<table><tr><td>End of training period</td><td>22-12</td><td>23-01</td><td>23-02</td><td>23-03</td><td>23-04</td><td>23-05</td><td>23-06</td><td>23-07</td><td>23-08</td><td>23-09</td><td>23-10</td><td>23-11</td><td>23-12</td><td>24-01</td></tr><tr><td>Number of Portfolios</td><td>2</td><td>9</td><td>3</td><td>2</td><td>3</td><td>2</td><td>4</td><td>1</td><td>5</td><td>2</td><td>4</td><td>5</td><td></td><td>2</td></tr><tr><td>SR Optimised</td><td>1.3</td><td>2.6</td><td>0.6</td><td>0.8</td><td>-0.2</td><td>-0.2</td><td>1.1</td><td>2.0</td><td>0.4</td><td>0.3</td><td>-2.6</td><td>1.2</td><td>-0.3</td><td>2.6</td></tr><tr><td>US Benchmark</td><td>1.9</td><td>4.1</td><td>0.9</td><td>1.2</td><td>-0.3</td><td>-0.3</td><td>1.8</td><td>2.7</td><td>-0.2</td><td>-0.9</td><td>-5.7</td><td>0.7</td><td>-0.4</td><td>4.0</td></tr></table>

Table 3. Summary of annualised portfolio returns (in percentage) in the in-sample training periods(in the SPY only case)

<table><tr><td>Testing period</td><td>23-01</td><td>23-02</td><td>23-03</td><td>23-04</td><td>23-05</td><td>23-06</td><td>23-07</td><td>23-08</td><td>23-09</td><td>23-10</td><td>23-11</td><td>23-12</td><td>24-01</td><td>24-02</td></tr><tr><td>SR Optimised</td><td>56.0</td><td>-29.8</td><td>32.2</td><td>8.5</td><td>2.8</td><td>51.5</td><td>28.1</td><td>-13.6</td><td>-22.2</td><td>-16.2</td><td>54.2</td><td>34.6</td><td>26.8</td><td>35.4</td></tr><tr><td>US Benchmark</td><td>81.9</td><td>-48.3</td><td>51.2</td><td>14.6</td><td>3.9</td><td>74.4</td><td>43.2</td><td>-22.4</td><td>-73.8</td><td>-30.7</td><td>107.0</td><td>56.8</td><td>33.3</td><td>61.6</td></tr></table>

Table 4. Summary of annualised portfolio returns (in percentage) in the out-of-sample training periods (in the SPY only case)

![](images/19b92866d4eb7ea510db902d81cbccd98e2358343911e4fb95216e4116412524.jpg)  
Figure 4. Portfolio returns for US optimised portfolio against its Benchmark in the out of sample period

In Table 3, we present the in-sample training period performance and its optimised return, comparing against the benchmark. Total number of portfolios $( r )$ in each of the rolling training period is reported in the first row, followed by the annualised return in the period in the second row. There is a general moderation of return, compared to the benchmark, which underlines the nature that stationary portfolio choices ought not to be profitable by themselves, even for the optimised in-sample combination. We proceed into the testing period, which are reported in Table 4 — there was a significant upward trend in the market in those period, as can be seen in the benchmark. Albeit, the optimised portfolio keeps up the general trend of up-tick while preserving lower negative returns in phases of downturns. In Figure 4, we summarise the cumulative returns in all testing period and plot against the benchmark. The volatility has clearly become lower, with cumulative return also being less than the benchmark.

Out of sample Sharpe ratio of the optimised portfolio for the period is reported at 1.96, while the benchmark is at 1.70. Maximum drawdown of the portfolio is at $4 . 5 \%$ while the benchmark is at $1 0 . 7 \%$ . More analyses of drawdown and risk management are discussed in section 5.

# 4.3 US and European markets

![](images/49524f9128ee664a228262afb96421757ba546582e3f81c060d0570441bf516d.jpg)  
Figure 5. Portfolio returns for jointly optimised portfolio against its Benchmark in the out of sample period

We now enlarge the asset space to include European markets. In this setting, $b = 2$ , and $\Theta$ is imposed to be such that $\theta _ { r + 1 } , \theta _ { r + 2 } \in ( \frac { 1 } { 4 } , \frac { 1 } { 2 } )$ . This is a moderated version to the US only case, wherein the benchmark composition should be at least a quarter each (totalled to half, which is the restriction in the US only case), while not allowing any one to fully dominate the portfolio.

There are two ways to optimise such a combination of two markets. The first being taking all three sets of estimations (the one in US market, the one in European market, and the one in the joint estimation) and treat all portfolios into the same optimisation control — this is annotated as "SR Optimised I" in Table 5 and Figure 5. In this case, the size of $r$ varies from 6 to 21, with detailed compositions being reported in the first three rows in Table 5. The other way is to take the joint estimations only (i.e. the joint portfolios as reported in Table 5), as arguably the joint estimations capture the entire dataset already, albeit the statistical risk of modelling a vastly sparse model (recall that the features in joint asset space being more than twice as much as the US only case, and that the Π matrix would then make the dimension to be more than four times as much). In this case, $r$ varies from 1 to 16, and certainly strictly less than the first way of combination, as the candidate portfolios are a subset of the first way’s. This method is annotated as "SR Optimised II" in the reporting results.

<table><tr><td>End of training period</td><td>22-12</td><td>23-01</td><td>23-02</td><td>23-03</td><td>23-04</td><td>23-05</td><td>23-06</td><td>23-07</td><td>23-08</td><td>23-09</td><td>23-10</td><td>23-11</td><td>23-12</td><td>24-01</td></tr><tr><td>US Portfolios</td><td>2</td><td>9</td><td>3</td><td>2</td><td>3</td><td>2</td><td>4</td><td>1</td><td>2</td><td>2</td><td>4</td><td>5</td><td></td><td>2</td></tr><tr><td>European Portfolios</td><td>4</td><td>2</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>2</td><td>2</td><td>1</td><td>3</td><td>5</td><td>7</td></tr><tr><td>Joint Portfolios</td><td>2</td><td>10</td><td>2</td><td>16</td><td>2</td><td>14</td><td>7</td><td>6</td><td>1</td><td>5</td><td>3</td><td>3</td><td>3</td><td>2</td></tr><tr><td>SR Optimised I</td><td>2.0</td><td>4.5</td><td>3.3</td><td>1.6</td><td>1.7</td><td>0.2</td><td>1.0</td><td>1.1</td><td>-0.4</td><td>0.5</td><td>-4.2</td><td>0.7</td><td>-0.5</td><td>2.0</td></tr><tr><td>SR Optimised II</td><td>2.0</td><td>4.4</td><td>3.3</td><td>1.6</td><td>1.6</td><td>0.2</td><td>1.0</td><td>1.2</td><td>-0.7</td><td>-0.2</td><td>-4.2</td><td>0.2</td><td>-0.3</td><td>2.4</td></tr><tr><td>European Benchmark</td><td>2.9</td><td>6.5</td><td>6.1</td><td>3.2</td><td>3.3</td><td>0.2</td><td>0.8</td><td>0.9</td><td>-1.9</td><td>-0.3</td><td>-5.6</td><td>-1.2</td><td>-1.4</td><td>1.3</td></tr><tr><td>Testing period</td><td>23-01</td><td>23-02</td><td>23-03</td><td>23-04</td><td>23-05</td><td>23-06</td><td>23-07</td><td>23-08</td><td>23-09</td><td>23-10</td><td>23-11</td><td>23-12</td><td>24-01</td><td>24-02</td></tr><tr><td>SR Optimised I</td><td>47.3</td><td>-1.6</td><td>12.6</td><td>14.0</td><td>-14.5</td><td>24.7</td><td>17.5</td><td>-5.5</td><td>-38.8</td><td>-17.4</td><td>72.3</td><td>30.0</td><td>23.5</td><td>24.4</td></tr><tr><td>SR Optimised I</td><td>48.6</td><td>0.3</td><td>14.5</td><td>14.0</td><td>-13.2</td><td>25.0</td><td>17.6</td><td>-8.5</td><td>-41.7</td><td>-17.7</td><td>71.9</td><td>39.3</td><td>13.3</td><td>38.4</td></tr><tr><td>European Benchmark</td><td>56.8</td><td>24.0</td><td>0.5</td><td>26.6</td><td>-27.8</td><td>19.9</td><td>31.0</td><td>-7.9</td><td>-23.6</td><td>-35.6</td><td>74.8</td><td>38.3</td><td>33.7</td><td>32.1</td></tr></table>

Remark: SR Optimised I refers to the portfolio optimised using all 3 types (US, European and Joint) whereas SR Optimised II refers to the one optimised using only Joint Portfolios. Table 5. Summary of annualised portfolio returns (in percentage) in the in-sample training period and out-of-sample testing period (in the joint asset space)

Both optimised portfolios depict a similar trend as was observed in the US only case, wherein they capture the general trend while having less volatility and drawdown. The difference between the two SR optimised portfolios are rather small — this means the US-only or European-only portfolios are relatively redundant when it comes to the optimal combination into Sharpe ratio optimisation.

Out of sample Sharpe ratio of the portfolios are 1.76 and 1.80 for I and II respectively, comparing to a benchmark of 1.7 and 1.42 for the US and European indices. Max drawdown for the two portfolios are reported at $5 . 2 \%$ and $5 . 9 \%$ respectively, whereas the ones for US and European indices are $1 0 . 7 \%$ and $6 . 7 \%$ .

In this subsection, we may nominally observe an interesting phenomenon wherein extension into a wider European market does not further improve the Sharpe ratio and Max Drawdown compared to the US market alone. However, this may be explained by general poorer performance compare to the US market, as the index does have a weaker Sharpe ratio by itself, so the extension may be understood as a cost of diversification in the relevant period. Furthermore, the optimised portfolio still outperforms both indices despite having to include at least $\textstyle { \frac { 1 } { 4 } }$ of the European index, underlining the strength of such optimised portfolio.

# 5 Further Application to Risk Management

# 5.1 Drawdown reduction

There are no mentioning of pure volatility minimisation or drawdown reduction in the objective function as per Equation 13, however, what can be explicitly observed from the result is that not only Sharpe ratio, in out-of-sample, outperforms the benchmarks, so do the risk metrics — here we focus on the drawdown.

![](images/c415f92037a16a18f9ef08f3307dcf57d50cdb5308cc773e41af61266ab9983e.jpg)  
Figure 6. Further illustration of drawdown: US and joint optimised portfolio against US benchmark

In Figure 6, we present the cumulative returns of SPY and the two SR optimised portfolios, one in the US-only asset space and another as the SR Optimised I case in the joint asset space. The running drawdown can be seen in the bottom plot, induced from the running max in the upper plot.

From the running drawdown, a clear indication can be drawn to the benefit of low-volatility combination with cointegrated portfolio, wherein the optimised portfolios managed to half the drawdown in many running cases, and achieving no significant idiosyncratic downside.

<table><tr><td>End of training period</td><td>22-12</td><td>23-01</td><td>23-02</td><td>23-03</td><td>23-04</td><td>23-05</td><td>23-06</td><td>23-07</td><td>23-08</td><td>23-09</td><td>23-10</td><td>23-11</td><td>23-12</td><td>24-01</td></tr><tr><td>Mean volatility</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>of candidate portfolios</td><td>2.1</td><td>1.9</td><td>0.7</td><td>0.7</td><td>1.0</td><td>0.8</td><td>0.9</td><td>0.9</td><td>6.4</td><td>1.5</td><td>5.3</td><td>1.9</td><td>4.4</td><td>3.5</td></tr><tr><td>Minimised Volatility</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>Average Benchmark</td><td>19.3</td><td>19.3</td><td>19.2</td><td>19.5</td><td>19.5</td><td>19.4</td><td>19.5</td><td>19.4</td><td>19.5</td><td>19.4</td><td>19.5</td><td>19.4</td><td>19.1</td><td>18.7</td></tr><tr><td>Testing period</td><td>23-01</td><td>23-02</td><td>23-03</td><td>23-04</td><td>23-05</td><td>23-06</td><td>23-07</td><td>23-08</td><td>23-09</td><td>23-10</td><td>23-11</td><td>23-12</td><td>24-01</td><td>24-02</td></tr><tr><td>Mean volatility</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>of candidate portfolios</td><td>2.3</td><td>2.0</td><td>1.0</td><td>1.1</td><td>1.3</td><td>0.9</td><td>1.4</td><td>1.0</td><td>3.7</td><td>1.7</td><td>4.0</td><td>1.8</td><td>3.0</td><td>2.6</td></tr><tr><td>Minimised Volatility</td><td>0.5</td><td>0.3</td><td>0.5</td><td>0.2</td><td>0.4</td><td>0.2</td><td>0.2</td><td>0.3</td><td>0.3</td><td>0.9</td><td>0.3</td><td>0.2</td><td>0.5</td><td>0.3</td></tr><tr><td>Average Benchmark</td><td>14.1</td><td>13.9</td><td>20.7</td><td>10.2</td><td>13.5</td><td>11.6</td><td>12.2</td><td>12.8</td><td>12.2</td><td>15.2</td><td>12.0</td><td>9.3</td><td>12.6</td><td>12.1</td></tr></table>

Table 6. Summary of portfolio volatility (in percentage) in the in-sample and out-of-sample testing periods, with joint benchmark being the average of US and European benchmarks

# 5.2 Volatility minimisation

In the preceding discussions, we focused on Equation 13 as the objective. Here, instead, we trial an alternative objective where we only focus on volatility minimisation. Particularly, consider

$$
\mathrm { m i n i m i s e ~ } \mathbb { E } _ { t \in \mathsf { t r a i n } } \left[ \sigma ( \Psi ( \theta ) ) _ { t } \right]
$$

instead of Equation 13. We do this to both the US-only asset space and joint asset space, with the $\Theta$ space relaxed into $\bar { \theta _ { r + 1 : r + b } } \in ( 0 , 1 ) ^ { b }$ instead of having a minimum holding requirement — demanding less passive holdings to see how far the cointegrated portfolios stretch to the volatility reduction.

In Table 6, results are presented in both the training and testing periods — the training volatility is brought down to below $0 . 0 1 \%$ from combining the $r$ many candidate portfolios. Though this is not preserved to testing, the same portfolio still yields more than half of the mean volatility in the candidate portfolios, which are already extremely low compared to the Benchmark.

We concatenate all testing periods and summarise the performance in Figure 7. Visually, the return can be seen as markedly smooth and drastically less volatile than benchmarks. The Sharpe ratio for US only is obtained at 2.26 and for the joint one being 2.40, while maximum drawdown are $0 . 7 \%$ and $0 . 3 \%$ respectively, which are more than $9 0 \%$ cut from the benchmarks.

![](images/717976317d70dbfa17b74c502c42505f0ea0f637ff3ba3d30a27efd5dd17552e.jpg)  
Figure 7. Volatility minimisation: US and jointly optimised portfolio against benchmarks

# 6 Discussions and Conclusion

# 6.1 Summary of results

To summarise various portfolio findings in this paper, we first recall section 3 for the reported volatility in stationary portfolios — the volatility amongst most of the portfolios are markedly lower than benchmark, which also reflect in a flat cumulative return. This motivates investigations in section 4 to mix the cointegrated portfolio with some elements of passive strategies. A summary of all the Sharpe ratio and Max Drawdown reported in the out-of-sample periods is presented in Table 7, as well as the extended investigation into pure volatility minimisation exercise in section 5.

<table><tr><td></td><td>Sharpe ratio Max Drawdown</td></tr><tr><td>section 4 US only</td><td>1.96</td></tr><tr><td>section 4 US and European</td><td>4.5% 1.76 5.2%</td></tr><tr><td>section 4 Joint only</td><td>1.80</td></tr><tr><td>section 5 US only</td><td>2.26</td></tr><tr><td>section 5 US and European</td><td>0.7% 2.4 0.3%</td></tr><tr><td>US Benchmark</td><td>1.70</td></tr><tr><td>European Benchmark</td><td>10.7% 1.42</td></tr></table>

Table 7. Summary of Sharpe ratio and Maximum Drawdown in out-of-sample testing periods

The inclusion of cointegrated portfolios has induced lowered return, but matched with much lower volatility — as they imply a higher Sharpe ratio in various settings of asset space. Drawdown analysis is further extended to showcase the cointegrated portfolios as potential instruments for risk management. We further extend this into a venue of pure volatility minimisation exercise and find both Sharpe ratio and Max Drawdown to be reported at best amongst all.

# 6.2 Methodological extension

Ongoing extensions are investigated on the methodological side, both in the models and in algorithms. For instance, one may criticise the model (as per section 2) to be not fully Bayesian, as it rests on a partial least square pre-estimation. Though the rationale has been that it reduces the complexity of rank determination into sparsity of the $R$ matrix, we are in search of more Bayesian approaches to fully express the model in Bayesian methods. This may involve other literature such as [11] for high dimensional matrix estimation.

# 6.3 More financial applications

There is admittedly less attention to potential practical problems arising from the cointegrated portfolios, such as foreign exchange rates (we purely took the returns for all assets, albeit the European stocks are quoted in local currencies), transaction costs, and adjustments of assets in the relevant asset space. Nonetheless, these should not affect the main result as we investigate the stocks with high market capitalisation and that the indices only have minor adjustments of components per quarter. Additionally, as the re-estimation hence rebalancing are done on a monthly basis, the risk and cost of over-trading is mitigated from the start. Therefore, these extensions are relevant, but more towards the implementations by practitioners.

[13] Liu Ziyin, Kentaro Minami, and Kentaro Imajo. 2022. Theoretically Motivated Data Augmentation and Regularization for Portfolio Construction. Association for Computing Machinery, New York, NY, USA.

# References

[1] Ray Bai, Veronika Rockova, and Edward I George. 2021. Spike-andslab meets lasso: A review of the spike-and-slab lasso. Handbook of Bayesian Variable Selection (2021), 81–108.   
[2] Gabriel Francisco Borrageiro, Nick Firoozye, and Paolo Barucca. 2022. Sequential asset ranking in nonstationary time series. Association for Computing Machinery, New York, NY, USA.   
[3] Joshua Brodie, Ingrid Daubechies, Christine De Mol, Domenico Giannone, and Ignace Loris. 2009. Sparse and stable Markowitz portfolios. Proceedings of the National Academy of Sciences 106, 30 (2009), 12267– 12272.   
[4] Robert F. Engle and C. W. J. Granger. 1987. Co-Integration and Error Correction: Representation, Estimation, and Testing. Econometrica 55, 2 (1987), 251–276.   
[5] Chong Liang and Melanie Schienle. 2019. Determination of vector error correction models in high dimensions. Journal of econometrics 208, 2 (2019), 418–441.   
[6] Harry Markowitz. 1952. Portfolio Selection. The Journal of Finance 7, 1 (1952), 77–91.   
[7] Yiming Peng and Vadim Linetsky. 2022. Portfolio Selection: A Statistical Learning Approach. Association for Computing Machinery, New York, NY, USA.   
[8] John Van Der Hoek Robert J. Elliott and William P. Malcolm. 2005. Pairs trading. Quantitative Finance 5, 3 (2005), 271–276.   
[9] Veronika Rockova and Edward I. George. 2018. The Spike-and-Slab LASSO. J. Amer. Statist. Assoc. 113, 521 (2018), 431–444.   
[10] Edward Turner and Mihai Cucuringu. 2023. Graph Denoising Networks: A Deep Learning Framework for Equity Portfolio Construction. Association for Computing Machinery, New York, NY, USA.   
[11] Maoran Xu and Leo L Duan. 2023. Bayesian inference with the l 1-ball prior: solving combinatorial problems with exact zeros. Journal of the Royal Statistical Society Series B: Statistical Methodology 85, 5 (2023), 1538–1560.   
[12] Parley Ruogu Yang and Alexander Y Shestopaloff. 2023. Bayesian Analysis of High Dimensional Vector Error Correction Model. arXiv:2312.17061v2 (2023).